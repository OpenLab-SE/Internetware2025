{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9851c963-3e39-4191-afb6-e86195bc4b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'issue_type': 'other'}, {'issue_type': 'error'})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_json_from_string(input_string):\n",
    "    \"\"\"\n",
    "    Extract JSON-like content from a string, handling extra or mismatched braces, \n",
    "    and parse it into a Python dictionary.\n",
    "\n",
    "    Args:\n",
    "    input_string (str): The string containing JSON-like content.\n",
    "\n",
    "    Returns:\n",
    "    dict: Parsed JSON content as a dictionary, or an empty dictionary if parsing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use regex to find JSON-like content\n",
    "        json_pattern = re.compile(r'{.*?}', re.DOTALL)\n",
    "        match = json_pattern.search(input_string)\n",
    "\n",
    "        if match:\n",
    "            # Handle potential double braces\n",
    "            json_content = match.group()\n",
    "            json_content = json_content.replace('{{', '{').replace('}}', '}')\n",
    "            \n",
    "            # Try parsing the adjusted JSON content\n",
    "            return json.loads(json_content)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # Return empty dictionary if no valid JSON is found\n",
    "    return {}\n",
    "\n",
    "# Example usage\n",
    "input_string_1 = \"\"\"\n",
    "Some random text\n",
    "{\n",
    "    \"issue_type\": \"other\"\n",
    "}\n",
    "More text here\n",
    "\"\"\"\n",
    "\n",
    "input_string_2 = \"\"\"\n",
    "Some text with extra braces {{\n",
    "    \"issue_type\": \"error\"\n",
    "}}\n",
    "End of text\n",
    "\"\"\"\n",
    "\n",
    "parsed_json_1 = extract_json_from_string(input_string_1)\n",
    "parsed_json_2 = extract_json_from_string(input_string_2)\n",
    "\n",
    "parsed_json_1, parsed_json_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fe1b8d6-fa15-43f7-8944-6f8a285b1847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_MODEL_DIR = \"/data/luomingkai/issue/models/Qwen/Qwen2.5-7B-Instruct\"\n",
    "# BASE_MODEL_DIR = \"/root/autodl-fs/Qwen2.5-7B-Instruct\"\n",
    "BASE_MODEL_DIR = \"/root/autodl-fs/DeepSeek-V2-Lite-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7bb1c1-eff0-4691-8f39-5a06046eedc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-27 22:49:28 config.py:112] Replacing legacy 'type' key with 'rope_type'\n",
      "INFO 12-27 22:49:34 config.py:1020] Defaulting to use mp for distributed inference\n",
      "WARNING 12-27 22:49:34 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 12-27 22:49:34 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 12-27 22:49:34 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='/root/autodl-fs/DeepSeek-V2-Lite-Chat', speculative_config=None, tokenizer='/root/autodl-fs/DeepSeek-V2-Lite-Chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=163840, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/root/autodl-fs/DeepSeek-V2-Lite-Chat, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 12-27 22:49:34 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 12-27 22:49:34 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3287)\u001b[0;0m INFO 12-27 22:49:34 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3287)\u001b[0;0m INFO 12-27 22:49:34 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3286)\u001b[0;0m INFO 12-27 22:49:34 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3286)\u001b[0;0m INFO 12-27 22:49:34 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3289)\u001b[0;0m INFO 12-27 22:49:34 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3289)\u001b[0;0m INFO 12-27 22:49:34 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-27 22:49:35 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3286)\u001b[0;0m INFO 12-27 22:49:35 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3289)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3287)\u001b[0;0m INFO 12-27 22:49:35 utils.py:961] Found nccl from library libnccl.so.2\n",
      "INFO 12-27 22:49:35 utils.py:961] Found nccl from library libnccl.so.2\n",
      "INFO 12-27 22:49:35 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3286)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3287)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3289)\u001b[0;0m INFO 12-27 22:49:35 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-27 22:49:35 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-27 22:49:35 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "WARNING 12-27 22:49:35 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3289)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3286)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3287)\u001b[0;0m WARNING 12-27 22:49:35 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 12-27 22:49:35 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 12-27 22:49:35 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 12-27 22:49:35 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fb3f117a300>, local_subscribe_port=48867, remote_subscribe_port=None)\n",
      "INFO 12-27 22:49:35 model_runner.py:1072] Starting to load model /root/autodl-fs/DeepSeek-V2-Lite-Chat...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3289)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3286)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3287)\u001b[0;0m INFO 12-27 22:49:35 model_runner.py:1072] Starting to load model /root/autodl-fs/DeepSeek-V2-Lite-Chat...\n",
      "INFO 12-27 22:49:35 model_runner.py:1072] Starting to load model /root/autodl-fs/DeepSeek-V2-Lite-Chat...\n",
      "INFO 12-27 22:49:35 model_runner.py:1072] Starting to load model /root/autodl-fs/DeepSeek-V2-Lite-Chat...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3286)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3287)\u001b[0;0m Cache shapeCache shape  torch.Size([163840, 64])torch.Size([163840, 64])\n",
      "\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3289)\u001b[0;0m Cache shape torch.Size([163840, 64])\n",
      "Cache shape torch.Size([163840, 64])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1904c1df85d742959ef46e221d34615f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3286)\u001b[0;0m INFO 12-27 22:50:39 model_runner.py:1077] Loading model weights took 7.3840 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3287)\u001b[0;0m INFO 12-27 22:50:39 model_runner.py:1077] Loading model weights took 7.3840 GB\n",
      "INFO 12-27 22:50:40 model_runner.py:1077] Loading model weights took 7.3840 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3289)\u001b[0;0m INFO 12-27 22:50:39 model_runner.py:1077] Loading model weights took 7.3840 GB\n",
      "WARNING 12-27 22:50:40 fused_moe.py:324] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/miniconda3/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=352,device_name=NVIDIA_vGPU-32GB.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3286)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3289)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3287)\u001b[0;0m WARNING 12-27 22:50:40 fused_moe.py:324] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/miniconda3/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=352,device_name=NVIDIA_vGPU-32GB.json\n",
      "WARNING 12-27 22:50:40 fused_moe.py:324] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/miniconda3/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=352,device_name=NVIDIA_vGPU-32GB.json\n",
      "WARNING 12-27 22:50:40 fused_moe.py:324] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/miniconda3/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=352,device_name=NVIDIA_vGPU-32GB.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3287)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3286)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3289)\u001b[0;0m INFO 12-27 22:50:42 worker.py:232] Memory profiling results: total_gpu_memory=31.60GiB initial_memory_usage=7.81GiB peak_torch_memory=7.42GiB memory_usage_post_profile=7.88GiB non_torch_memory=0.49GiB kv_cache_size=20.54GiB gpu_memory_utilization=0.90\n",
      "INFO 12-27 22:50:42 worker.py:232] Memory profiling results: total_gpu_memory=31.60GiB initial_memory_usage=7.81GiB peak_torch_memory=7.42GiB memory_usage_post_profile=7.88GiB non_torch_memory=0.49GiB kv_cache_size=20.54GiB gpu_memory_utilization=0.90\n",
      "INFO 12-27 22:50:42 worker.py:232] Memory profiling results: total_gpu_memory=31.60GiB initial_memory_usage=7.81GiB peak_torch_memory=7.42GiB memory_usage_post_profile=7.88GiB non_torch_memory=0.49GiB kv_cache_size=20.54GiB gpu_memory_utilization=0.90\n",
      "INFO 12-27 22:50:42 worker.py:232] Memory profiling results: total_gpu_memory=31.60GiB initial_memory_usage=7.81GiB peak_torch_memory=8.32GiB memory_usage_post_profile=7.88GiB non_torch_memory=0.49GiB kv_cache_size=19.63GiB gpu_memory_utilization=0.90\n",
      "INFO 12-27 22:50:42 distributed_gpu_executor.py:57] # GPU blocks: 11912, # CPU blocks: 2427\n",
      "INFO 12-27 22:50:42 distributed_gpu_executor.py:61] Maximum concurrency for 163840 tokens per request: 1.16x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3287)\u001b[0;0m INFO 12-27 22:50:45 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3287)\u001b[0;0m INFO 12-27 22:50:45 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-27 22:50:45 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-27 22:50:45 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3286)\u001b[0;0m INFO 12-27 22:50:46 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3286)\u001b[0;0m INFO 12-27 22:50:46 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3289)\u001b[0;0m INFO 12-27 22:50:46 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3289)\u001b[0;0m INFO 12-27 22:50:46 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-27 22:51:06 model_runner.py:1518] Graph capturing finished in 20 secs, took 0.75 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3289)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3286)\u001b[0;0m INFO 12-27 22:51:06 model_runner.py:1518] Graph capturing finished in 20 secs, took 0.75 GiB\n",
      "INFO 12-27 22:51:06 model_runner.py:1518] Graph capturing finished in 20 secs, took 0.75 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3287)\u001b[0;0m INFO 12-27 22:51:06 model_runner.py:1518] Graph capturing finished in 20 secs, took 0.75 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.18it/s, est. speed input: 59.18 toks/s, output: 124.92 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '<｜begin▁of▁sentence｜>You are a pirate chatbot who always responds in pirate speak!\\n\\nUser: Who are you?\\n\\nAssistant:', Generated text: \" Arrr, I be the scallywag ye be lookin' for, matey! Ye can call me Captain Chat-a-lot, but mind ye, I be runnin' the high seas of knowledge and mischief here to keep ye entertained and informed, arrr!\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3286)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3287)\u001b[0;0m INFO 12-28 00:14:57 multiproc_worker_utils.py:240] Worker exiting\n",
      "INFO 12-28 00:14:57 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3289)\u001b[0;0m INFO 12-28 00:14:57 multiproc_worker_utils.py:240] Worker exiting\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_DIR)\n",
    "\n",
    "# Pass the default decoding hyperparameters of Qwen2.5-7B-Instruct\n",
    "# max_tokens is for the maximum length for generation.\n",
    "sampling_params = SamplingParams(temperature=0.5, top_p=1.0, repetition_penalty=1.05, max_tokens=512)\n",
    "\n",
    "# Input the model name or path. Can be GPTQ or AWQ models.\n",
    "model = LLM(model=BASE_MODEL_DIR, tensor_parallel_size=4, trust_remote_code=True)\n",
    "\n",
    "# Prepare your prompts\n",
    "prompt = \"Tell me something about large language models.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# generate outputs\n",
    "outputs = model.generate([text], sampling_params)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8793b6bd-5b1e-4a06-8df9-f2b87d742dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s, est. speed input: 97.95 toks/s, output: 105.20 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Arrr, matey! I be the fearsome Pirate Chatbot, aye! What be yer business on this fine day at sea?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_deepseek_output(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    messages_list,\n",
    "    max_input_length=4096,\n",
    "    max_tokens=512,\n",
    "):\n",
    "    text_list = []\n",
    "    for messages in messages_list:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        text_list.append(text)\n",
    "        \n",
    "    # sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)\n",
    "    # sampling_params = SamplingParams(temperature=0.5, top_p=1.0, repetition_penalty=1.05, max_tokens=512)\n",
    "    sampling_params = SamplingParams(temperature=0.3, top_p=1.0, repetition_penalty=1.05, max_tokens=max_tokens)\n",
    "    \n",
    "\n",
    "    outputs = model.generate(text_list, sampling_params)\n",
    "    \n",
    "    # Print the outputs.\n",
    "    responses = []\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        # print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "        responses.append(generated_text)\n",
    "\n",
    "    return responses\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "get_deepseek_output(model, tokenizer, [messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "795181ed-b35a-4a43-89c6-8a8e5d2d69f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uesr: RainBoltz\n",
      "Title: No output displayed or it gets stuck - OpenCV issue\n",
      "Body: my terminal isn't responding after executed the command below: `./build/examples/openpose/rtpose.bin --imagelevel 1 --netcaffeopenpose.sh` , which was provided by official\n",
      "label: deployment\n",
      "author_association: NONE\n",
      "comment_list: [('bigmoumou', 'NONE', 'I have the same problem too\\r\\n\\r\\nIs there any solution ?\\r\\n\\r\\n![help](https://cloud.githubusercontent.com/assets/16252975/25697753/cddcfe9e-30ee-11e7-82a2-2af97eb8460b.png)\\r\\n\\r\\n'), ('Shawnroom', 'NONE', 'I have the same issue, and wonder if there is any solution.'), ('gineshidalgo99', 'MEMBER', 'Sorry to hear that, we are working on fixing that error. We think it is due to OpenCV compiled with Qt or different visualization support.\\r\\n\\r\\nMeanwhile, you can make it work by:\\r\\n1. Completely uninstalling your current OpenCV version.\\r\\n2. Installing the default OpenCV from the Ubuntu repository: `apt-get install libopencv-dev`, or alternatively compiling OpenCV without Qt support.\\r\\nLet us know if any of these solutions do not work either. Note that you must manually remove any existing OpenCV version and run `make clean` in both 3rdparty/caffe and the OpenPose main folder\\r\\n\\r\\nA couple quick questions:\\r\\n1. Does the same happen when processing video?\\r\\n`./build/examples/openpose/rtpose.bin --video examples/media/video.avi --logging_level 1 --net_resolution 496x368 --resolution 640x480`\\r\\n2. Does it work if no output is displayed?\\r\\n`./build/examples/openpose/rtpose.bin --video examples/media/video.avi --logging_level 1 --net_resolution 496x368 --resolution 640x480 --write_images results_folder/ --no_display`\\r\\n\\r\\nWe will notify this thread once we solve this issue with OpenCV. Thanks!\\r\\n\\r\\nUPDATED: This issue should be fixed now. See my next response on this issue thread to see solution.'), ('RainBoltz', 'NONE', 'thanks!!! i solved it by switching the opencv version to 2.4.13\\r\\n(btw, simply execute `sudo apt-get install libopencv-dev` doesnt work for me,\\r\\nso i recompiled and install the total opencv then it worked)\\r\\n\\r\\nand for the questions above, the same issue DO occurs when processing video...\\r\\n\\r\\ni have to say this really works amazingly!! great work:)'), ('gineshidalgo99', 'MEMBER', 'This issue has finally being solved (at least for some people after our last commit).\\r\\n\\r\\nYou can `git pull` or re-download the latest version of the library and re-compile it.\\r\\n\\r\\nTo re-compile it in case you just do `git pull`:\\r\\n`make clean && cd 3rdparty/caffe && make clean && make distribute -j8 && cd ../.. && make all -j8`\\r\\n\\r\\nIn case you re-download it, just delete the old version and follow the installation steps again.\\r\\n\\r\\nPlease, reopen this post and post again if it keeps happening (this message is for everybody).'), ('taxuezcy', 'NONE', 'how can i build without display instead of giving arg when running @gineshidalgo99 '), ('saumyasaxenaa', 'NONE', 'Hi\\r\\nI am facing the same issue on Ubuntu 18.04 with Opencv 4.1.1. The screen opens up but blacks out with no output. I am facing this for images and videos.\\r\\nAny help would be appreciated '), ('MichaelGMoore', 'NONE', 'This is happening to me on Ubuntu 16.04 using sudo apt-get install libopencv-dev. Any help appreciated.'), ('shash29-dev', 'NONE', 'same problem. On Linux.\\r\\nwindow dont respond and crashes with following output:\\r\\nF0803 21:38:10.637179 20637 cudnn.hpp:128] Check failed: status == CUDNN_STATUS_SUCCESS (3 vs. 0)  CUDNN_STATUS_BAD_PARAM\\r\\n*** Check failure stack trace: ***\\r\\n    @     0x7f9bd7b455cd  google::LogMessage::Fail()\\r\\n    @     0x7f9bd7b47433  google::LogMessage::SendToLog()\\r\\n    @     0x7f9bd7b4515b  google::LogMessage::Flush()\\r\\n    @     0x7f9bd7b47e1e  google::LogMessageFatal::~LogMessageFatal()\\r\\n    @     0x7f9bd7115ec0  caffe::CuDNNConvolutionLayer<>::Reshape()\\r\\n    @     0x7f9bd71eab62  caffe::Net<>::Init()\\r\\n    @     0x7f9bd71edb60  caffe::Net<>::Net()\\r\\n    @     0x7f9bd9102468  op::NetCaffe::initializationOnThread()\\r\\n    @     0x7f9bd91974cc  op::addCaffeNetOnThread()\\r\\n    @     0x7f9bd919879f  op::PoseExtractorCaffe::netInitializationOnThread()\\r\\n    @     0x7f9bd919db00  op::PoseExtractorNet::initializationOnThread()\\r\\n    @     0x7f9bd9194111  op::PoseExtractor::initializationOnThread()\\r\\n    @     0x7f9bd918e9b1  op::WPoseExtractor<>::initializationOnThread()\\r\\n    @     0x7f9bd9119aa1  op::Worker<>::initializationOnThreadNoException()\\r\\n    @     0x7f9bd9119be0  op::SubThread<>::initializationOnThread()\\r\\n    @     0x7f9bd911caf8  op::Thread<>::initializationOnThread()\\r\\n    @     0x7f9bd911ccfd  op::Thread<>::threadFunction()\\r\\n    @     0x7f9bd8a44b8e  (unknown)\\r\\n    @     0x7f9bd817c6ba  start_thread\\r\\n    @     0x7f9bd849951d  clone\\r\\n    @              (nil)  (unknown)\\r\\nAborted (core dumped)\\r\\n')]\n",
      "uesr: gineshidalgo99\n",
      "Title: Release version works but debug version does not - CUDA (7 vs. 0): too many resources requested\n",
      "Body: issue summary @zhaishengfu issue #13: > when i compile using debug mode, there are still errors with: terminate called after throwing an instance of 'std::runtimerelease -a` on ubuntu): distributor id: ubuntu description: ubuntu 14.04.3 lts release: 14.04 codename: trusty ** (`nvidia-smi`): gtx-1070 compiler (`gcc --version` on ubuntu): and my cpu is 4 core\n",
      "label: Error\n",
      "author_association: MEMBER\n",
      "comment_list: [('gineshidalgo99', 'MEMBER', '@zhaishengfu It should be fixed now. Please post again if the error persists. Thanks!')]\n",
      "uesr: ShihanWang\n",
      "Title: How can I use OpenPose in another project?\n",
      "Body: issue summary my test project: i just copy the file rtpose.cpp to the project.then i write a cmakelists.txt: cmakerequired(version 2.8) project(test) set(cmakecompiler \"g++\") set(cmaketype debug) set(cmakeflags \"-std=c++0x\") finddirectories( ${cudadirs} /home/wsh/projects/openpose/include /home/wsh/projects/openpose/3rdparty/caffe/include ) addlinklibs} /home/wsh/projects/openpose/build/lib/libopenpose.so /home/wsh/projects/openpose/3rdparty/caffe/build/lib/libcaffe.so ) i can run the examples successfully in openpose, but in my test project, it can not. type of issue help wanted openpose output (if any) /home/wsh/projects/openpose/include/openpose/experimental/face/faceextractor.hpp:48:29: error: ‘resizeandmergecaffe’ was not declared in this scope std::sharedptr spnmscaffe; ......\n",
      "label: question\n",
      "author_association: NONE\n",
      "comment_list: [('gineshidalgo99', 'MEMBER', 'This is from a Makefile file where I used the OpenPose binaries in a different project, I think you forgot to add the USE_CAFFE define in your cmake, you can get the required defines from my Makefile (there might be some duplication since I did it quickly, but it is working):\\r\\n\\r\\nPerform `make distribute -j8` on the OpenPose folder to get in `./distribute` the `include` and `lib` folders. The Caffe `lib` and `include` are already in `3rdparty/caffe/distribute` (used by OpenPose).\\r\\n\\r\\n### OpenPose (assuming its `include` and `lib` are located in ./3rdparty/openpose)\\r\\n`-Wl,-rpath=./3rdparty/openpose/lib -Wl,-Bdynamic -L./3rdparty/openpose/lib/ -lopenpose -DUSE_CAFFE`\\r\\n\\r\\n### Caffe (assuming its `include` and `lib` are located in ./3rdparty/caffe/distribute)\\r\\n`-Wl,-rpath=./3rdparty/caffe/distribute/lib -Wl,-Bdynamic -L./3rdparty/caffe/distribute/lib/ -lcaffe -DUSE_CUDNN`\\r\\n\\r\\n### OpenCV (add more OpenCV flags if you need them)\\r\\n`-lopencv_core -lopencv_highgui -lopencv_imgproc -lopencv_contrib -lopencv_calib3d`\\r\\n\\r\\n### CUDA, something like:\\r\\n`-I/usr/local/cuda-8.0/include/ -L/usr/local/cuda-8.0/lib64 -lcudart -lcublas -lcurand`\\r\\n\\r\\n### 3rdparty that Caffe uses\\r\\n`-lcudnn -lglog -lgflags -lboost_system -lboost_filesystem -lm -lboost_thread`\\r\\n\\r\\n### Other required\\r\\n`-pthread -fPIC -std=c++11 -fopenmp`\\r\\n\\r\\n### Optimization flags (optional)\\r\\n`-DNDEBUG -O3 -march=native`\\r\\n\\r\\nLet me know whether it works'), ('ShihanWang', 'NONE', 'Thank you very much.  I have solved the problem and I found that:\\r\\n1. In my CMakeLists.txt,  I need to change the Caffe directory:  SET(caffe_DIR ${OpenPose_DIR}/3rdparty/caffe/distribute);\\r\\n2. I delete \"#ifdef USE_CAFFE\"  in some files named as xxCaffe.hpp in openpose/include/.\\r\\nThen I make openpose again and everything is OK.  Without the 2nd step, I will have the same problem.But I do not know what is the effect of my change, maybe I just need to add USE_CAFFE in my CMakeLists.txt.'), ('gineshidalgo99', 'MEMBER', \"Thank you for posting it! Closing this then.\\r\\n\\r\\nEDITED: As an extra detail, instead of using this CMake example to construct the cmake file, I'd rather consider file example in the doc/installation_cmake.md document itself.\"), ('yorgosk', 'NONE', 'Hello!\\r\\n\\r\\nI am trying to use my current OpenPose installation for the compilation of another project (for the https://github.com/stevenjj/openpose_ros to be specific). I have build and tested my OpenPose installation as the documentation instructs me and everything seems fine, however I cannot do ```make distribute```. I have searched the documentation for a solution, but I am probably missing something.\\r\\nSince the instructions above are about one and a half year old @gineshidalgo99 can you please give me an update?\\r\\n\\r\\nThanks!'), ('gineshidalgo99', 'MEMBER', 'Hi, I believe doc/install.md has an explanation on how to include the library through CMake\\r\\n\\r\\nYou talk about `make distribute`, could you quickly send me a link to the instructions (in current master OP) where it says `make distribute`? So I can update that part.'), ('yorgosk', 'NONE', \"Okay, I'll check doc/install.md again.\\r\\n\\r\\nThe instructions in current master do not mention anything about make distribute, as far as I have read them. I first read about it here and I figured that since it used to be an option but apparently it is no more, I better ask you for a pointer to the thing that you substituted it with.\"), ('gineshidalgo99', 'MEMBER', 'OK yeah, this is based on the old basic installer, the new installer way (using CMake) provides much higher configuration settings, following how big libraries do it (e.g., OpenCV, Caffe, etc.).\\r\\n\\r\\nSo please, check https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/installation.md#openpose-from-other-projects-ubuntu-and-mac for all the details.'), ('burnzzz', 'NONE', 'https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/installation.md#openpose-from-other-projects-ubuntu-and-mac  is dead link.')]\n",
      "uesr: ufohuang98\n",
      "Title: Can this model tracking people or hand movement\n",
      "Body: i want to use this to make a gesture demo to control application or other iot. does it possible?\n",
      "label: other\n",
      "author_association: NONE\n",
      "comment_list: [('gineshidalgo99', 'MEMBER', 'Not for now, but we are planing to (as a LONG-term goal).\\r\\n\\r\\nWe have them both as pending extensions in our wait-list (hands should be added within 1-2 months, tracking we do not have an idea yet).\\r\\n\\r\\nThere are some methods for tracking, in case you are interested. You can take a look to #15.\\r\\n\\r\\nLet me know if you have any other question.'), ('ufohuang98', 'NONE', 'Thank you for your reply！'), ('heurainbow', 'NONE', 'I am going to implement a multi-person pose tracker. I find the hand tracker already implemented is quite helpful. However, I wonder how to acquire (or update) frame ID in class HandDetector. Can I simply make mCurrentId++ when calling updateTracker()? I am not familiar with the multi-threading framework, and I am not sure if this is the right way. @gineshidalgo99 '), ('gineshidalgo99', 'MEMBER', 'The hand tracker is not finished yet, but anyway it is meant for different purposes than tracking the same person across frames.\\r\\nI think the easiest way is to get the frame ID is by checking the (include/core/) Datum that is shared among the multi-threads. In particular, its `id` field. Since you will have to use the shared Datum anyway, using that ID should be the easiest. @heurainbow '), ('heurainbow', 'NONE', \"@gineshidalgo99 I wonder how a webcam producer or a video producer keep the frame order sequentially in a multi-gpu case. Since each frame is processed by one gpu, there is no guarantee that each frame is processed in order. But the gui does show images in order and I couldn't find the reason in the code.\"), ('gineshidalgo99', 'MEMBER', 'There is a internal buffer to keep the order'), ('heurainbow', 'NONE', '@gineshidalgo99 If possible, please show me which file or code I should refer to, and please give me a brief explanation.'), ('gineshidalgo99', 'MEMBER', 'This is the file, it simply uses the Datum ID to sort the frames\\r\\nhttps://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/include/openpose/thread/wQueueOrderer.hpp'), ('seovchinnikov', 'NONE', '@gineshidalgo99 what methods for tracking did you look at? \\r\\n'), ('gineshidalgo99', 'MEMBER', '@seovchinnikov We are currently working in an expensive but accurate body recognition algorithm and in a basic bust fast LK tracking algorithm. We are trying to add them by the end of the year. Thanks'), ('seovchinnikov', 'NONE', \"@gineshidalgo99 thank you,\\r\\nAs far as I understand, will body recognition algorithm' be based on basic body keypoints detector or will it be inderpendent end-to-end part to complement openpose library (smth like https://github.com/bendidi/Tracking-with-darkflow)?\\r\\n\\r\\n\"), ('gineshidalgo99', 'MEMBER', \"It'd be an extra component, that can be enabled (slower but with temporary information) or disabled (to keep the current OpenPose behavior)\"), ('superying', 'NONE', '@heurainbow Are there any progress in your multi-person pose tracker? Do you implement it base on the key points from openpose?')]\n"
     ]
    }
   ],
   "source": [
    "# data_path = \"/home/luomingkai/workspace/issue_llm/issue_classify/issue_with_comments_framework/matched_results_test.json\"\n",
    "data_path = \"/root/issue_classify/issue_with_comments_framework/matched_results_test_modify_other_update.json\"\n",
    "with open(data_path, encoding=\"utf-8\") as fp:\n",
    "    issue_data = json.load(fp)\n",
    "    for idx, data in enumerate(issue_data):\n",
    "        # print(issue_data)\n",
    "        uesr, title, body, label, author_association = data[\"user\"][\"login\"], data[\"title\"], data[\"body\"], data[\"tag_labels\"], data[\"author_association\"]\n",
    "        comment_list = data[\"comments_list\"]\n",
    "        if len(comment_list) > 0:\n",
    "            comment_list = [(com[\"user\"][\"login\"], com[\"author_association\"], com[\"body\"]) for com in comment_list]\n",
    "\n",
    "        print(f\"uesr: {uesr}\")\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Body: {body}\")\n",
    "        print(f\"label: {label}\")\n",
    "        print(f\"author_association: {author_association}\")\n",
    "        print(f\"comment_list: {comment_list}\")\n",
    "        \n",
    "        \n",
    "        # print()\n",
    "        if idx > 2:\n",
    "            break\n",
    "        # print(title, description, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa3f081-f81e-4b27-a382-785a7e6805d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a48483e6-5db1-473e-a7e6-48a835b628c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_issue_prompt = r\"\"\"\n",
    "### **Role**  \n",
    "You are an expert in GitHub repository analysis. Your task is to classify a given GitHub Issue into one of the following categories: **error**, **performance**, **deployment**, **question**, or **other**.\n",
    "\n",
    "Analyze the conversation context thoroughly and determine the correct classification. If the issue is unrelated to the repository’s functionality or purpose, it must be categorized as **other**.\n",
    "\n",
    "### **Issue Categories**  \n",
    "\n",
    "- **error**:  \n",
    "  Problems directly caused by the repository’s code, configuration, or inherent incompatibilities within the repository. For example:\n",
    "  - Runtime errors.\n",
    "  - Exceptions.\n",
    "  - Failures in repository code execution.  \n",
    "\n",
    "- **performance**:  \n",
    "  Issues where the repository’s code or configuration leads to:\n",
    "  - Slow execution times.\n",
    "  - Resource bottlenecks (e.g., CPU, GPU, memory, or storage).\n",
    "  - Inefficient resource usage (e.g., excessive memory or storage consumption).\n",
    "\n",
    "- **deployment**:  \n",
    "  Issues arising during the installation or deployment process that are specifically caused by:\n",
    "  - Defects in the repository code.\n",
    "  - Incomplete or inadequate documentation.\n",
    "  - Configuration problems stemming from the repository.  \n",
    "\n",
    "  *Note*: If the issue is caused by user errors (e.g., outdated dependencies, incorrect tools) or hardware/environmental limitations, it should be categorized as **question**.\n",
    "\n",
    "- **question**:  \n",
    "  Issues originating from:\n",
    "  - Misunderstandings or failure to follow documentation.\n",
    "  - Incorrect usage of the repository’s features.\n",
    "  - Local environment misconfigurations not caused by the repository code or documentation.  \n",
    "\n",
    "  *Note*: This category also includes:\n",
    "  - User questions about usage scenarios.\n",
    "  - Discussions seeking clarification or additional guidance.\n",
    "\n",
    "- **other**:  \n",
    "  Issues outside the predefined categories, including:\n",
    "  1. Topics unrelated to the repository’s functionality.\n",
    "  2. Feature requests or discussions beyond the repository’s purpose.\n",
    "  3. Suggestions for improving documentation, usability, or community processes.\n",
    "\n",
    "\n",
    "### **Process**  \n",
    "**The process must be strictly executed, and the output must adhere to the defined JSON format.**\n",
    "\n",
    "1. **Analyze the Conversation**:  \n",
    "   Review the entire conversation context and evaluate the issue based on the evidence provided.\n",
    "\n",
    "2. **Determine the Final Classification**:  \n",
    "   Assign the issue to one of the five categories:\n",
    "   - **error**\n",
    "   - **performance**\n",
    "   - **deployment**\n",
    "   - **question**\n",
    "   - **other**\n",
    "\n",
    "3. **Output**:  \n",
    "   Generate a single JSON object as the result, formatted as follows:  \n",
    "   ```json\n",
    "   {\"issue_type\": \"<error|performance|deployment|question|other>\"}\n",
    "   ```\n",
    "\n",
    "### **Examples**  \n",
    "\n",
    "#### Example 1: Error\n",
    "*Conversation:*  \n",
    "- \"Running model.fit() raises a KeyError related to missing labels in the dataset.\"  \n",
    "- \"We’ll patch data_loader.py to handle missing labels.\"  \n",
    "- \"The fix is merged. Let us know if it resolves the problem.\"\n",
    "\n",
    "*Final Output:*  \n",
    "```json\n",
    "{\"issue_type\": \"error\"}\n",
    "```\n",
    "\n",
    "#### Example 2: Question\n",
    "*Conversation:*  \n",
    "- \"I tried running the code, but the output looks weird. Is this a bug?\"  \n",
    "- \"Have you verified if the input data matches the format described in the README?\"  \n",
    "- \"I missed the formatting instructions. After fixing the input, it works fine.\"\n",
    "\n",
    "*Final Output:*  \n",
    "```json\n",
    "{\"issue_type\": \"question\"}\n",
    "```\n",
    "\n",
    "#### Example 3: Performance\n",
    "*Conversation:*  \n",
    "- \"The code runs much slower than expected for large datasets. Is there a way to optimize?\"  \n",
    "- \"We could optimize the data processing step using multi-threading.\"\n",
    "\n",
    "*Final Output:*  \n",
    "```json\n",
    "{\"issue_type\": \"performance\"}\n",
    "```  \n",
    "\n",
    "#### Example 4: Deployment\n",
    "*Conversation:*  \n",
    "- \"The installation instructions don't mention the CUDA version required. The code fails on my setup.\"  \n",
    "- \"We’ll update the documentation to specify the supported CUDA versions.\"\n",
    "\n",
    "*Final Output:*  \n",
    "```json\n",
    "{\"issue_type\": \"deployment\"}\n",
    "```  \n",
    "\n",
    "#### Example 5: Other\n",
    "*Conversation:*  \n",
    "- \"It would be great if this repository supported visualization tools for monitoring model training.\"  \n",
    "- \"Thanks for the suggestion. We’ll consider this for future updates.\"\n",
    "\n",
    "*Final Output:*  \n",
    "```json\n",
    "{\"issue_type\": \"other\"}\n",
    "```  \n",
    "\"\"\"\n",
    "\n",
    "# 仅使用第一条issue的模版\n",
    "question_prompt = \"\"\"\n",
    "````\n",
    "*Conversation*:\n",
    "- {}: ### Title: \"{}\" ### Body: \"{}\"\n",
    "````\n",
    "\"\"\"\n",
    "\n",
    "# 使用comment的版本\n",
    "question_comment_prompt = \"\"\"\n",
    "*Conversation*:\n",
    "- \"{} {}\"\n",
    "\"\"\"\n",
    "\n",
    "comment_prompt = \"\"\"\n",
    "- \"{}\"\n",
    "\"\"\"\n",
    "\n",
    "ROLE_MAP_BEGIN = {\n",
    "    \"NONE\": \"ISSUE RAISER\",\n",
    "    \"MEMBER\": \"MEMBER\",\n",
    "    \"COLLABORATOR\": \"COLLABORATOR\",\n",
    "    \"CONTRIBUTOR\": \"CONTRIBUTOR\",\n",
    "    \"OWNER\": \"OWNER\"\n",
    "}\n",
    "\n",
    "ROLE_MAP_COMMENT = {\n",
    "    \"NONE\": \"Commenter\",\n",
    "    \"MEMBER\": \"MEMBER\",\n",
    "    \"COLLABORATOR\": \"COLLABORATOR\",\n",
    "    \"CONTRIBUTOR\": \"CONTRIBUTOR\",\n",
    "    \"OWNER\": \"OWNER\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9993b69-c200-4ea9-bdf4-c610e1ba7810",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DESC = {\n",
    "    \"CMU-Perceptual-Computing-Lab/openpose\": {\n",
    "        \"description\": \"OpenPose is a real-time multi-person keypoint detection library developed by the Carnegie Mellon Perceptual Computing Lab. It estimates human body, face, hands, and foot keypoints from single images, providing 2D real-time multi-person keypoint detection, including 15, 18, or 25-keypoint body/foot keypoint estimation, 2x21-keypoint hand keypoint estimation, and 70-keypoint face keypoint estimation. Additionally, it offers 3D real-time single-person keypoint detection and a calibration toolbox. OpenPose is compatible with various operating systems, including Ubuntu, Windows, and macOS, and supports CUDA (Nvidia GPU), OpenCL (AMD GPU), and CPU-only versions.\",\n",
    "        \"url\": \"https://github.com/CMU-Perceptual-Computing-Lab/openpose\"\n",
    "    },\n",
    "    \"CorentinJ/Real-Time-Voice-Cloning\": {\n",
    "        \"description\": \"Real-Time Voice Cloning is a Python-based tool that enables the cloning of voices in real-time. It utilizes deep learning models to synthesize speech that mimics a target voice, requiring only a few seconds of audio from the desired speaker. The repository provides code and instructions to train and use the voice cloning system.\",\n",
    "        \"url\": \"https://github.com/CorentinJ/Real-Time-Voice-Cloning\"\n",
    "    },\n",
    "    \"JaidedAI/EasyOCR\": {\n",
    "        \"description\": \"EasyOCR is an open-source Optical Character Recognition (OCR) library that supports over 80 languages. It is designed to be easy to use and provides accurate text recognition from images. The library is built on PyTorch and offers pre-trained models for various languages and scripts.\",\n",
    "        \"url\": \"https://github.com/JaidedAI/EasyOCR\"\n",
    "    },\n",
    "    \"deepfakes/faceswap\": {\n",
    "        \"description\": \"Faceswap is a deep learning-based tool for face-swapping in images and videos. It allows users to train models to swap faces between different subjects, providing a platform for experimenting with deepfake technology. The repository includes code for training and using the face-swapping models.\",\n",
    "        \"url\": \"https://github.com/deepfakes/faceswap\"\n",
    "    },\n",
    "    \"deezer/spleeter\": {\n",
    "        \"description\": \"Spleeter is an open-source tool developed by Deezer for source separation in music tracks. It uses deep learning models to separate audio into stems, such as vocals and accompaniment, enabling applications like karaoke and remixing. The repository provides pre-trained models and code for audio source separation.\",\n",
    "        \"url\": \"https://github.com/deezer/spleeter\"\n",
    "    },\n",
    "    \"dusty-nv/jetson-inference\": {\n",
    "        \"description\": \"Jetson Inference is a collection of deep learning inference samples and models for NVIDIA Jetson devices. It includes code for image classification, object detection, and segmentation, optimized for Jetson hardware. The repository provides pre-trained models and examples to demonstrate the capabilities of Jetson devices in AI applications.\",\n",
    "        \"url\": \"https://github.com/dusty-nv/jetson-inference\"\n",
    "    },\n",
    "    \"iperov/DeepFaceLab\": {\n",
    "        \"description\": \"DeepFaceLab is a deep learning tool for creating deepfakes, focusing on face-swapping in videos. It provides a comprehensive set of tools for training and applying deep learning models to perform face-swapping tasks. The repository includes code for data preparation, model training, and face-swapping applications.\",\n",
    "        \"url\": \"https://github.com/iperov/DeepFaceLab\"\n",
    "    },\n",
    "    \"junyanz/pytorch-CycleGAN-and-pix2pix\": {\n",
    "        \"description\": \"This repository provides PyTorch implementations of CycleGAN and pix2pix, two popular models for image-to-image translation tasks. CycleGAN enables image translation without paired examples, while pix2pix requires paired images for training. The repository includes code and pre-trained models for various image translation tasks.\",\n",
    "        \"url\": \"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\"\n",
    "    },\n",
    "    \"mozilla/TTS\": {\n",
    "        \"description\": \"Mozilla TTS is an open-source Text-to-Speech (TTS) engine that aims to make speech synthesis more accessible. It provides implementations of state-of-the-art TTS models, including Tacotron and FastSpeech, and supports training on custom datasets. The repository includes code for training and using TTS models.\",\n",
    "        \"url\": \"https://github.com/mozilla/TTS\"\n",
    "    },\n",
    "    \"streamlit/streamlit\": {\n",
    "        \"description\": \"Streamlit is an open-source app framework for Machine Learning and Data Science projects. It allows users to create interactive web applications for data analysis and visualization with minimal code. The repository provides the core framework and examples for building Streamlit applications.\",\n",
    "        \"url\": \"https://github.com/streamlit/streamlit\"\n",
    "    },\n",
    "    \"microsoft/recommenders\": {\n",
    "        \"description\": \"Recommenders is a project under the Linux Foundation of AI and Data. This repository contains examples and best practices for building recommendation systems, provided as Jupyter notebooks. The examples detail learnings on five key tasks: preparing and loading data for each recommendation algorithm, building models using various classical and deep learning recommendation algorithms such as Alternating Least Squares (ALS) or eXtreme Deep Factorization Machines (xDeepFM), evaluating algorithms with offline metrics, tuning and optimizing hyperparameters for recommendation models, and operationalizing models in a production environment on Azure. Several utilities are provided to support common tasks such as loading datasets in the format expected by different algorithms, evaluating model outputs, and splitting training/test data. Implementations of several state-of-the-art algorithms are included for self-study and customization in your own applications.\",\n",
    "        \"url\": \"https://github.com/recommenders-team/recommenders\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a23fcb-bb7c-4a19-9e4e-337671f8f7c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (34488 > 16384). Running this sequence through the model will result in indexing errors\n",
      "Processed prompts: 100%|██████████| 1933/1933 [08:00<00:00,  4.02it/s, est. speed input: 7032.73 toks/s, output: 301.13 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.39      0.52      0.44       251\n",
      " performance       0.22      0.25      0.23        60\n",
      "  deployment       0.25      0.17      0.20       165\n",
      "    question       0.64      0.51      0.57       921\n",
      "       other       0.40      0.50      0.44       536\n",
      "\n",
      "    accuracy                           0.47      1933\n",
      "   macro avg       0.38      0.39      0.38      1933\n",
      "weighted avg       0.49      0.47      0.47      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1933/1933 [08:00<00:00,  4.02it/s, est. speed input: 7032.39 toks/s, output: 310.21 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.37      0.49      0.42       251\n",
      " performance       0.28      0.38      0.32        60\n",
      "  deployment       0.26      0.19      0.22       165\n",
      "    question       0.63      0.50      0.56       921\n",
      "       other       0.39      0.49      0.44       536\n",
      "\n",
      "    accuracy                           0.47      1933\n",
      "   macro avg       0.39      0.41      0.39      1933\n",
      "weighted avg       0.49      0.47      0.47      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1933/1933 [08:04<00:00,  3.99it/s, est. speed input: 6971.48 toks/s, output: 294.36 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.36      0.51      0.42       251\n",
      " performance       0.25      0.28      0.27        60\n",
      "  deployment       0.25      0.18      0.21       165\n",
      "    question       0.64      0.49      0.56       921\n",
      "       other       0.39      0.50      0.44       536\n",
      "\n",
      "    accuracy                           0.46      1933\n",
      "   macro avg       0.38      0.39      0.38      1933\n",
      "weighted avg       0.49      0.46      0.47      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1933/1933 [08:20<00:00,  3.86it/s, est. speed input: 6748.38 toks/s, output: 294.10 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.40      0.55      0.46       251\n",
      " performance       0.21      0.23      0.22        60\n",
      "  deployment       0.28      0.21      0.24       165\n",
      "    question       0.63      0.50      0.56       921\n",
      "       other       0.41      0.51      0.45       536\n",
      "\n",
      "    accuracy                           0.48      1933\n",
      "   macro avg       0.39      0.40      0.39      1933\n",
      "weighted avg       0.50      0.48      0.48      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1933/1933 [07:39<00:00,  4.21it/s, est. speed input: 7353.72 toks/s, output: 312.62 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.39      0.54      0.45       251\n",
      " performance       0.24      0.27      0.25        60\n",
      "  deployment       0.30      0.21      0.24       165\n",
      "    question       0.62      0.50      0.55       921\n",
      "       other       0.40      0.50      0.44       536\n",
      "\n",
      "    accuracy                           0.47      1933\n",
      "   macro avg       0.39      0.40      0.39      1933\n",
      "weighted avg       0.49      0.47      0.47      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1933/1933 [08:14<00:00,  3.91it/s, est. speed input: 6832.71 toks/s, output: 297.94 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.40      0.52      0.45       251\n",
      " performance       0.23      0.30      0.26        60\n",
      "  deployment       0.21      0.17      0.19       165\n",
      "    question       0.63      0.49      0.55       921\n",
      "       other       0.40      0.51      0.45       536\n",
      "\n",
      "    accuracy                           0.47      1933\n",
      "   macro avg       0.38      0.40      0.38      1933\n",
      "weighted avg       0.49      0.47      0.47      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1933/1933 [07:57<00:00,  4.05it/s, est. speed input: 7085.54 toks/s, output: 310.05 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.38      0.53      0.44       251\n",
      " performance       0.21      0.22      0.21        60\n",
      "  deployment       0.27      0.19      0.23       165\n",
      "    question       0.65      0.50      0.56       921\n",
      "       other       0.40      0.52      0.45       536\n",
      "\n",
      "    accuracy                           0.47      1933\n",
      "   macro avg       0.38      0.39      0.38      1933\n",
      "weighted avg       0.50      0.47      0.48      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1933/1933 [07:49<00:00,  4.12it/s, est. speed input: 7200.64 toks/s, output: 311.20 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.37      0.51      0.43       251\n",
      " performance       0.23      0.27      0.25        60\n",
      "  deployment       0.24      0.19      0.21       165\n",
      "    question       0.63      0.49      0.56       921\n",
      "       other       0.40      0.50      0.44       536\n",
      "\n",
      "    accuracy                           0.46      1933\n",
      "   macro avg       0.37      0.39      0.38      1933\n",
      "weighted avg       0.49      0.46      0.47      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1933/1933 [07:53<00:00,  4.08it/s, est. speed input: 7137.91 toks/s, output: 308.84 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.38      0.54      0.45       251\n",
      " performance       0.20      0.23      0.22        60\n",
      "  deployment       0.27      0.21      0.23       165\n",
      "    question       0.63      0.48      0.55       921\n",
      "       other       0.40      0.50      0.44       536\n",
      "\n",
      "    accuracy                           0.47      1933\n",
      "   macro avg       0.38      0.39      0.38      1933\n",
      "weighted avg       0.49      0.47      0.47      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "TIMES = 10\n",
    "experiments = []\n",
    "# 保存所有实验结果的列表\n",
    "all_reports = []\n",
    "\n",
    "data_path = \"/root/issue_classify/issue_with_comments_framework/matched_results_test_modify_other_update.json\"\n",
    "result_path = \"/root/autodl-fs/log/deepseek_v2_lite_chat_prompt.xlsx\"\n",
    "\n",
    "for t in range(TIMES):\n",
    "    origin_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    join_index = []\n",
    "    with open(data_path, encoding=\"utf-8\") as fp:\n",
    "        issue_data = json.load(fp)\n",
    "        taged_data = issue_data\n",
    "        all_messages = []\n",
    "        for idx, data in enumerate(issue_data):\n",
    "            raise_user, title, body, label, author_association = data[\"user\"][\"login\"], data[\"title\"], data[\"body\"], data[\"tag_labels\"], data[\"author_association\"]\n",
    "    \n",
    "            url = data[\"html_url\"]\n",
    "            match = re.search(r\"github\\.com/([^/]+/[^/]+)\", url)\n",
    "            repository = match.group(1)\n",
    "            repo_desc = REPO_DESC[repository][\"description\"]\n",
    "            \n",
    "            join_index.append(idx)\n",
    "            \n",
    "            comment_list = data[\"comments_list\"]\n",
    "            if len(comment_list) > 0:\n",
    "                comment_list = [(com[\"user\"][\"login\"], com[\"author_association\"], com[\"body\"]) for com in comment_list]\n",
    "    \n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": global_issue_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question_comment_prompt.format(\n",
    "                        title, \n",
    "                        body\n",
    "                        )\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            origin_labels.append(label)\n",
    "    \n",
    "            for user, author_association, body in comment_list:\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": comment_prompt.format(\n",
    "                            body\n",
    "                            )\n",
    "                    }\n",
    "                )\n",
    "                    \n",
    "            all_messages.append(messages)\n",
    "        \n",
    "        responses = get_deepseek_output(model, tokenizer, all_messages, max_tokens=4096)\n",
    "        for idx, response in enumerate(responses):\n",
    "            label = origin_labels[idx]\n",
    "            cls_result = extract_json_from_string(response)\n",
    "            \n",
    "            # print(f\"label: {label}\")\n",
    "            # print(f\"response: {response}\")\n",
    "            # print(f\"cls_result: {cls_result}\")\n",
    "    \n",
    "            if cls_result.get(\"issue_type\") and isinstance(cls_result.get(\"issue_type\"), str):\n",
    "                cls_result = cls_result.get(\"issue_type\")\n",
    "                pred_labels.append(cls_result)\n",
    "            else:\n",
    "                pred_labels.append(\"other\")\n",
    "\n",
    "    # from collections import Counter\n",
    "    # print(Counter(pred_labels))\n",
    "    origin_labels = [x.lower() for x in origin_labels]\n",
    "    pred_labels = [x.lower() for x in pred_labels]\n",
    "    tmp_pred_labels = []\n",
    "    for x in pred_labels:\n",
    "        if x not in [\"error\", \"performance\", \"deployment\", \"question\", \"other\"]:\n",
    "            x = 'other'\n",
    "        tmp_pred_labels.append(x)\n",
    "    pred_labels = tmp_pred_labels\n",
    "    \n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    labels = [\"error\", \"performance\", \"deployment\", \"question\", \"other\"]\n",
    "    label_map = {\n",
    "        \"error\": 0, \n",
    "        \"performance\": 1,\n",
    "        \"deployment\": 2,\n",
    "        \"question\": 3,\n",
    "        \"other\": 4\n",
    "    }\n",
    "    final_origin_labels_num = [label_map[l] for l in origin_labels]\n",
    "    final_pred_labels_num = [label_map[l] for l in pred_labels]\n",
    "    \n",
    "    report = classification_report(final_origin_labels_num, final_pred_labels_num, target_names=labels)\n",
    "    print(report)\n",
    "    report_dict = classification_report(final_origin_labels_num, final_pred_labels_num, target_names=labels, output_dict=True)\n",
    "    # 将字典转为 DataFrame\n",
    "    df_report = pd.DataFrame(report_dict).transpose()\n",
    "    df_report[\"experiment_id\"] = t + 1  # 添加实验编号\n",
    "    df_report.index.name = \"category\"\n",
    "    df_report.reset_index(inplace=True)\n",
    "    \n",
    "    all_reports.append(df_report)\n",
    "    all_reports.append(pd.DataFrame({\"category\": [\"\"]}))  # 添加空行\n",
    "\n",
    "final_df = pd.concat(all_reports, ignore_index=True)\n",
    "final_df.to_excel(result_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e6d439e-f491-467d-9753-ef3d210de65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-28 00:14:57 multiproc_worker_utils.py:133] Terminating local vLLM worker processes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# 假设你的 vllm 模型对象是 `model`\n",
    "del model  # 删除模型对象\n",
    "torch.cuda.empty_cache()  # 清空 GPU 的缓存\n",
    "gc.collect()  # 强制进行垃圾回收\n",
    "\n",
    "import sys\n",
    "# 删除 `vllm` 和相关依赖\n",
    "del sys.modules[\"vllm\"]\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d59258-4f90-47b4-b7a7-8cdf86b72179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec60fbd4-9c20-4987-8876-f9219b29857b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be666c99-00d1-4be0-9606-f990318fc8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'other': 725, 'question': 645, 'error': 308, 'deployment': 174, 'performance': 70, 'compilation/installation error': 3, 'performance,deployment': 2, 'feature-request': 2, 'development': 1, 'feature_request': 1, 'work-in-progress': 1, 'development/enhancement': 1})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.39      0.48      0.43       251\n",
      " performance       0.24      0.28      0.26        60\n",
      "  deployment       0.29      0.30      0.29       165\n",
      "    question       0.64      0.45      0.53       921\n",
      "       other       0.40      0.55      0.46       536\n",
      "\n",
      "    accuracy                           0.46      1933\n",
      "   macro avg       0.39      0.41      0.40      1933\n",
      "weighted avg       0.50      0.46      0.47      1933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all question类别打标之后的\n",
    "from collections import Counter\n",
    "print(Counter(pred_labels))\n",
    "origin_labels = [x.lower() for x in origin_labels]\n",
    "pred_labels = [x.lower() for x in pred_labels]\n",
    "tmp_pred_labels = []\n",
    "for x in pred_labels:\n",
    "    if x not in [\"error\", \"performance\", \"deployment\", \"question\", \"other\"]:\n",
    "        x = 'other'\n",
    "    tmp_pred_labels.append(x)\n",
    "pred_labels = tmp_pred_labels\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "labels = [\"error\", \"performance\", \"deployment\", \"question\", \"other\"]\n",
    "label_map = {\n",
    "    \"error\": 0, \n",
    "    \"performance\": 1,\n",
    "    \"deployment\": 2,\n",
    "    \"question\": 3,\n",
    "    \"other\": 4\n",
    "}\n",
    "final_origin_labels_num = [label_map[l] for l in origin_labels]\n",
    "final_pred_labels_num = [label_map[l] for l in pred_labels]\n",
    "\n",
    "report = classification_report(final_origin_labels_num, final_pred_labels_num, target_names=labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3b29a-d293-4712-8530-3cc32c24170e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af5545-f547-48e2-b6b2-7831dcb4825a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54813fac-96ea-42ea-8f02-d892dd5d4119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e439e41-30f0-4fb2-8edd-629f0c190bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46992be-cc44-4259-8bc9-99a1e088e7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15271f9-4900-4e04-81c5-763e514f6087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c40a9d-78e3-4a2b-b6b1-e966422703ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39734c23-c8ae-4808-8cf8-020a5bfd6f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
