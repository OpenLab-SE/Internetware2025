{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eff6182-e0e0-4cab-9d8f-a90de40edfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'round': 1, 'classification': 'deployment', 'evidence': \"The ISSUE RAISER reported an error indicating that 'spleeter' is not recognized as a command, suggesting an issue with the installation or environment setup.\"}, {'round': 2, 'classification': 'deployment', 'evidence': 'The COLLABORATOR suggested running `python -m spleeter separate` as a workaround, indicating a possible issue with the installation process.'}, {'round': 3, 'classification': 'deployment', 'evidence': 'The COLLABORATOR suggested trying `spleeter.exe`, further confirming the issue is related to the installation or environment setup.'}, {'round': 4, 'classification': 'deployment', 'evidence': 'The COMMENTER suggested installing pyssl and other dependencies, which indicates a possible issue with the environment configuration.'}, {'round': 5, 'classification': 'deployment', 'evidence': 'Another COMMENTER reported the same issue with the longer command `python -m spleeter separate`, indicating a consistent issue with the installation or environment setup.'}, {'round': 6, 'classification': 'deployment', 'evidence': 'The COLLABORATOR identified a problem with the Conda environment installation, suggesting that the issue is related to the environment setup.'}, {'round': 7, 'classification': 'deployment', 'evidence': 'A COMMENTER detailed the steps they took to resolve the issue, including ensuring that the Conda environment was correctly set up and that the necessary modules were installed.'}, {'round': 8, 'classification': 'deployment', 'evidence': 'Another COMMENTER provided additional steps to ensure proper installation, including running the Anaconda command prompt as an administrator and ensuring Python was added to the PATH.'}, {'final_issue_type': 'deployment', 'combined_evidence': \"The issue revolves around the inability to recognize 'spleeter' as a command, which is typically related to installation or environment setup problems. Multiple users reported similar issues, and the COLLABORATOR and COMMENTERS suggested various steps to resolve the environment configuration, such as ensuring correct Conda environment setup and adding Python to the PATH.\"}]\n",
      "9\n",
      "{'final_issue_type': 'deployment', 'combined_evidence': \"The issue revolves around the inability to recognize 'spleeter' as a command, which is typically related to installation or environment setup problems. Multiple users reported similar issues, and the COLLABORATOR and COMMENTERS suggested various steps to resolve the environment configuration, such as ensuring correct Conda environment setup and adding Python to the PATH.\"}\n",
      "<class 'dict'>\n",
      "deployment\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_all_json_from_string(input_string):\n",
    "    \"\"\"\n",
    "    Extract all JSON-like content from a string and parse it into a list of Python dictionaries.\n",
    "\n",
    "    Args:\n",
    "    input_string (str): The string containing JSON-like content.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of parsed JSON content as dictionaries, or an empty list if no valid JSON is found.\n",
    "    \"\"\"\n",
    "    json_objects = []\n",
    "    try:\n",
    "        # Use regex to find all JSON-like content\n",
    "        json_pattern = re.compile(r'\\{.*?\\}', re.DOTALL)\n",
    "        matches = json_pattern.findall(input_string)\n",
    "\n",
    "        for match in matches:\n",
    "            try:\n",
    "                # Handle potential double braces and parse each JSON object\n",
    "                json_content = match.replace('{{', '{').replace('}}', '}')\n",
    "                parsed_json = json.loads(json_content)\n",
    "                json_objects.append(parsed_json)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip invalid JSON matches\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error during JSON extraction: {e}\")\n",
    "\n",
    "    return json_objects\n",
    "\n",
    "# Example usage\n",
    "input_string = \"\"\"\n",
    "response: ### Intermediate Results for Each Round:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"round\": 1,\n",
    "    \"classification\": \"deployment\",\n",
    "    \"evidence\": \"The ISSUE RAISER reported an error indicating that 'spleeter' is not recognized as a command, suggesting an issue with the installation or environment setup.\"\n",
    "}\n",
    "{\n",
    "    \"round\": 2,\n",
    "    \"classification\": \"deployment\",\n",
    "    \"evidence\": \"The COLLABORATOR suggested running `python -m spleeter separate` as a workaround, indicating a possible issue with the installation process.\"\n",
    "}\n",
    "{\n",
    "    \"round\": 3,\n",
    "    \"classification\": \"deployment\",\n",
    "    \"evidence\": \"The COLLABORATOR suggested trying `spleeter.exe`, further confirming the issue is related to the installation or environment setup.\"\n",
    "}\n",
    "{\n",
    "    \"round\": 4,\n",
    "    \"classification\": \"deployment\",\n",
    "    \"evidence\": \"The COMMENTER suggested installing pyssl and other dependencies, which indicates a possible issue with the environment configuration.\"\n",
    "}\n",
    "{\n",
    "    \"round\": 5,\n",
    "    \"classification\": \"deployment\",\n",
    "    \"evidence\": \"Another COMMENTER reported the same issue with the longer command `python -m spleeter separate`, indicating a consistent issue with the installation or environment setup.\"\n",
    "}\n",
    "{\n",
    "    \"round\": 6,\n",
    "    \"classification\": \"deployment\",\n",
    "    \"evidence\": \"The COLLABORATOR identified a problem with the Conda environment installation, suggesting that the issue is related to the environment setup.\"\n",
    "}\n",
    "{\n",
    "    \"round\": 7,\n",
    "    \"classification\": \"deployment\",\n",
    "    \"evidence\": \"A COMMENTER detailed the steps they took to resolve the issue, including ensuring that the Conda environment was correctly set up and that the necessary modules were installed.\"\n",
    "}\n",
    "{\n",
    "    \"round\": 8,\n",
    "    \"classification\": \"deployment\",\n",
    "    \"evidence\": \"Another COMMENTER provided additional steps to ensure proper installation, including running the Anaconda command prompt as an administrator and ensuring Python was added to the PATH.\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Final Combined Output:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"final_issue_type\": \"deployment\",\n",
    "    \"combined_evidence\": \"The issue revolves around the inability to recognize 'spleeter' as a command, which is typically related to installation or environment setup problems. Multiple users reported similar issues, and the COLLABORATOR and COMMENTERS suggested various steps to resolve the environment configuration, such as ensuring correct Conda environment setup and adding Python to the PATH.\"\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "parsed_json = extract_all_json_from_string(input_string)\n",
    "print(parsed_json)\n",
    "print(len(parsed_json))\n",
    "print(parsed_json[-1])\n",
    "print(type(parsed_json[-1]))\n",
    "print(parsed_json[-1].get('final_issue_type'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fe1b8d6-fa15-43f7-8944-6f8a285b1847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_MODEL_DIR = \"/data/luomingkai/issue/models/Qwen/Qwen2.5-7B-Instruct\"\n",
    "# BASE_MODEL_DIR = \"/root/autodl-fs/Qwen2.5-7B-Instruct\"\n",
    "BASE_MODEL_DIR = \"/root/autodl-fs/DeepSeek-V2-Lite-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7bb1c1-eff0-4691-8f39-5a06046eedc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-27 22:26:49 config.py:112] Replacing legacy 'type' key with 'rope_type'\n",
      "INFO 12-27 22:26:55 config.py:1020] Defaulting to use mp for distributed inference\n",
      "WARNING 12-27 22:26:55 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 12-27 22:26:55 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 12-27 22:26:55 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='/root/autodl-fs/DeepSeek-V2-Lite-Chat', speculative_config=None, tokenizer='/root/autodl-fs/DeepSeek-V2-Lite-Chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=163840, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/root/autodl-fs/DeepSeek-V2-Lite-Chat, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 12-27 22:26:55 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 12-27 22:26:55 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2639)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2637)\u001b[0;0m INFO 12-27 22:26:55 selector.py:135] Using Flash Attention backend.\n",
      "INFO 12-27 22:26:55 selector.py:135] Using Flash Attention backend.\n",
      "INFO 12-27 22:26:55 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2639)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2637)\u001b[0;0m INFO 12-27 22:26:55 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-27 22:26:55 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-27 22:26:55 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-27 22:26:56 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638)\u001b[0;0m INFO 12-27 22:26:56 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2637)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2639)\u001b[0;0m INFO 12-27 22:26:56 utils.py:961] Found nccl from library libnccl.so.2\n",
      "INFO 12-27 22:26:56 utils.py:961] Found nccl from library libnccl.so.2\n",
      "INFO 12-27 22:26:56 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2637)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2639)\u001b[0;0m INFO 12-27 22:26:56 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-27 22:26:56 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-27 22:26:56 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "WARNING 12-27 22:26:56 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2637)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2639)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638)\u001b[0;0m WARNING 12-27 22:26:56 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 12-27 22:26:56 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 12-27 22:26:56 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 12-27 22:26:56 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f7e35e52bd0>, local_subscribe_port=34073, remote_subscribe_port=None)\n",
      "INFO 12-27 22:26:56 model_runner.py:1072] Starting to load model /root/autodl-fs/DeepSeek-V2-Lite-Chat...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2637)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2639)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638)\u001b[0;0m INFO 12-27 22:26:56 model_runner.py:1072] Starting to load model /root/autodl-fs/DeepSeek-V2-Lite-Chat...\n",
      "INFO 12-27 22:26:56 model_runner.py:1072] Starting to load model /root/autodl-fs/DeepSeek-V2-Lite-Chat...\n",
      "INFO 12-27 22:26:56 model_runner.py:1072] Starting to load model /root/autodl-fs/DeepSeek-V2-Lite-Chat...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2637)\u001b[0;0m Cache shape torch.Size([163840, 64])\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2639)\u001b[0;0m Cache shapeCache shape  torch.Size([163840, 64])torch.Size([163840, 64])\n",
      "\n",
      "Cache shape torch.Size([163840, 64])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4a81b13e9f4ae483346d05167f5afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-27 22:27:45 model_runner.py:1077] Loading model weights took 7.3840 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2639)\u001b[0;0m INFO 12-27 22:27:46 model_runner.py:1077] Loading model weights took 7.3840 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638)\u001b[0;0m INFO 12-27 22:27:46 model_runner.py:1077] Loading model weights took 7.3840 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2637)\u001b[0;0m INFO 12-27 22:27:46 model_runner.py:1077] Loading model weights took 7.3840 GB\n",
      "WARNING 12-27 22:27:46 fused_moe.py:324] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/miniconda3/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=352,device_name=NVIDIA_vGPU-32GB.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2637)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2639)\u001b[0;0m WARNING 12-27 22:27:46 fused_moe.py:324] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/miniconda3/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=352,device_name=NVIDIA_vGPU-32GB.json\n",
      "WARNING 12-27 22:27:46 fused_moe.py:324] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/miniconda3/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=352,device_name=NVIDIA_vGPU-32GB.json\n",
      "WARNING 12-27 22:27:46 fused_moe.py:324] Using default MoE config. Performance might be sub-optimal! Config file not found at /root/miniconda3/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=352,device_name=NVIDIA_vGPU-32GB.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2637)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2639)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638)\u001b[0;0m INFO 12-27 22:27:48 worker.py:232] Memory profiling results: total_gpu_memory=31.60GiB initial_memory_usage=7.81GiB peak_torch_memory=7.42GiB memory_usage_post_profile=7.88GiB non_torch_memory=0.49GiB kv_cache_size=20.54GiB gpu_memory_utilization=0.90\n",
      "INFO 12-27 22:27:48 worker.py:232] Memory profiling results: total_gpu_memory=31.60GiB initial_memory_usage=7.81GiB peak_torch_memory=7.42GiB memory_usage_post_profile=7.88GiB non_torch_memory=0.49GiB kv_cache_size=20.54GiB gpu_memory_utilization=0.90\n",
      "INFO 12-27 22:27:48 worker.py:232] Memory profiling results: total_gpu_memory=31.60GiB initial_memory_usage=7.81GiB peak_torch_memory=7.42GiB memory_usage_post_profile=7.88GiB non_torch_memory=0.49GiB kv_cache_size=20.54GiB gpu_memory_utilization=0.90\n",
      "INFO 12-27 22:27:48 worker.py:232] Memory profiling results: total_gpu_memory=31.60GiB initial_memory_usage=7.81GiB peak_torch_memory=8.32GiB memory_usage_post_profile=7.88GiB non_torch_memory=0.49GiB kv_cache_size=19.63GiB gpu_memory_utilization=0.90\n",
      "INFO 12-27 22:27:48 distributed_gpu_executor.py:57] # GPU blocks: 11912, # CPU blocks: 2427\n",
      "INFO 12-27 22:27:48 distributed_gpu_executor.py:61] Maximum concurrency for 163840 tokens per request: 1.16x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638)\u001b[0;0m INFO 12-27 22:27:51 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2638)\u001b[0;0m INFO 12-27 22:27:51 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2639)\u001b[0;0m INFO 12-27 22:27:51 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2639)\u001b[0;0m INFO 12-27 22:27:51 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2637)\u001b[0;0m INFO 12-27 22:27:52 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2637)\u001b[0;0m INFO 12-27 22:27:52 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-27 22:27:52 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-27 22:27:52 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-27 22:28:11 model_runner.py:1518] Graph capturing finished in 19 secs, took 0.75 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2639)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2638)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2637)\u001b[0;0m INFO 12-27 22:28:11 model_runner.py:1518] Graph capturing finished in 20 secs, took 0.75 GiB\n",
      "INFO 12-27 22:28:11 model_runner.py:1518] Graph capturing finished in 20 secs, took 0.75 GiB\n",
      "INFO 12-27 22:28:11 model_runner.py:1518] Graph capturing finished in 20 secs, took 0.75 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s, est. speed input: 60.82 toks/s, output: 128.39 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '<｜begin▁of▁sentence｜>You are a pirate chatbot who always responds in pirate speak!\\n\\nUser: Who are you?\\n\\nAssistant:', Generated text: \" Arrr, I be the scallywag ye be lookin' for, matey! Ye can call me Captain Chat-a-lot, but mind ye, I be runnin' the high seas of knowledge and mischief here to keep ye entertained and informed, arrr!\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_DIR)\n",
    "\n",
    "# Pass the default decoding hyperparameters of Qwen2.5-7B-Instruct\n",
    "# max_tokens is for the maximum length for generation.\n",
    "sampling_params = SamplingParams(temperature=0.5, top_p=1.0, repetition_penalty=1.05, max_tokens=512)\n",
    "\n",
    "# Input the model name or path. Can be GPTQ or AWQ models.\n",
    "model = LLM(model=BASE_MODEL_DIR, tensor_parallel_size=4, trust_remote_code=True)\n",
    "\n",
    "# Prepare your prompts\n",
    "prompt = \"Tell me something about large language models.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# generate outputs\n",
    "outputs = model.generate([text], sampling_params)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8793b6bd-5b1e-4a06-8df9-f2b87d742dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.02it/s, est. speed input: 108.96 toks/s, output: 117.02 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Arrr, matey! I be the fearsome Pirate Chatbot, aye! What be yer business on this fine day at sea?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_deepseek_output(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    messages_list,\n",
    "    max_input_length=4096,\n",
    "    max_tokens=512,\n",
    "):\n",
    "    text_list = []\n",
    "    for messages in messages_list:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        text_list.append(text)\n",
    "        \n",
    "    # sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)\n",
    "    # sampling_params = SamplingParams(temperature=0.5, top_p=1.0, repetition_penalty=1.05, max_tokens=512)\n",
    "    sampling_params = SamplingParams(temperature=0.3, top_p=1.0, repetition_penalty=1.05, max_tokens=max_tokens)\n",
    "    \n",
    "\n",
    "    outputs = model.generate(text_list, sampling_params)\n",
    "    \n",
    "    # Print the outputs.\n",
    "    responses = []\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        # print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "        responses.append(generated_text)\n",
    "\n",
    "    return responses\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "get_deepseek_output(model, tokenizer, [messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "795181ed-b35a-4a43-89c6-8a8e5d2d69f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uesr: RainBoltz\n",
      "Title: No output displayed or it gets stuck - OpenCV issue\n",
      "Body: my terminal isn't responding after executed the command below: `./build/examples/openpose/rtpose.bin --imagelevel 1 --netcaffeopenpose.sh` , which was provided by official\n",
      "label: deployment\n",
      "author_association: NONE\n",
      "comment_list: [('bigmoumou', 'NONE', 'I have the same problem too\\r\\n\\r\\nIs there any solution ?\\r\\n\\r\\n![help](https://cloud.githubusercontent.com/assets/16252975/25697753/cddcfe9e-30ee-11e7-82a2-2af97eb8460b.png)\\r\\n\\r\\n'), ('Shawnroom', 'NONE', 'I have the same issue, and wonder if there is any solution.'), ('gineshidalgo99', 'MEMBER', 'Sorry to hear that, we are working on fixing that error. We think it is due to OpenCV compiled with Qt or different visualization support.\\r\\n\\r\\nMeanwhile, you can make it work by:\\r\\n1. Completely uninstalling your current OpenCV version.\\r\\n2. Installing the default OpenCV from the Ubuntu repository: `apt-get install libopencv-dev`, or alternatively compiling OpenCV without Qt support.\\r\\nLet us know if any of these solutions do not work either. Note that you must manually remove any existing OpenCV version and run `make clean` in both 3rdparty/caffe and the OpenPose main folder\\r\\n\\r\\nA couple quick questions:\\r\\n1. Does the same happen when processing video?\\r\\n`./build/examples/openpose/rtpose.bin --video examples/media/video.avi --logging_level 1 --net_resolution 496x368 --resolution 640x480`\\r\\n2. Does it work if no output is displayed?\\r\\n`./build/examples/openpose/rtpose.bin --video examples/media/video.avi --logging_level 1 --net_resolution 496x368 --resolution 640x480 --write_images results_folder/ --no_display`\\r\\n\\r\\nWe will notify this thread once we solve this issue with OpenCV. Thanks!\\r\\n\\r\\nUPDATED: This issue should be fixed now. See my next response on this issue thread to see solution.'), ('RainBoltz', 'NONE', 'thanks!!! i solved it by switching the opencv version to 2.4.13\\r\\n(btw, simply execute `sudo apt-get install libopencv-dev` doesnt work for me,\\r\\nso i recompiled and install the total opencv then it worked)\\r\\n\\r\\nand for the questions above, the same issue DO occurs when processing video...\\r\\n\\r\\ni have to say this really works amazingly!! great work:)'), ('gineshidalgo99', 'MEMBER', 'This issue has finally being solved (at least for some people after our last commit).\\r\\n\\r\\nYou can `git pull` or re-download the latest version of the library and re-compile it.\\r\\n\\r\\nTo re-compile it in case you just do `git pull`:\\r\\n`make clean && cd 3rdparty/caffe && make clean && make distribute -j8 && cd ../.. && make all -j8`\\r\\n\\r\\nIn case you re-download it, just delete the old version and follow the installation steps again.\\r\\n\\r\\nPlease, reopen this post and post again if it keeps happening (this message is for everybody).'), ('taxuezcy', 'NONE', 'how can i build without display instead of giving arg when running @gineshidalgo99 '), ('saumyasaxenaa', 'NONE', 'Hi\\r\\nI am facing the same issue on Ubuntu 18.04 with Opencv 4.1.1. The screen opens up but blacks out with no output. I am facing this for images and videos.\\r\\nAny help would be appreciated '), ('MichaelGMoore', 'NONE', 'This is happening to me on Ubuntu 16.04 using sudo apt-get install libopencv-dev. Any help appreciated.'), ('shash29-dev', 'NONE', 'same problem. On Linux.\\r\\nwindow dont respond and crashes with following output:\\r\\nF0803 21:38:10.637179 20637 cudnn.hpp:128] Check failed: status == CUDNN_STATUS_SUCCESS (3 vs. 0)  CUDNN_STATUS_BAD_PARAM\\r\\n*** Check failure stack trace: ***\\r\\n    @     0x7f9bd7b455cd  google::LogMessage::Fail()\\r\\n    @     0x7f9bd7b47433  google::LogMessage::SendToLog()\\r\\n    @     0x7f9bd7b4515b  google::LogMessage::Flush()\\r\\n    @     0x7f9bd7b47e1e  google::LogMessageFatal::~LogMessageFatal()\\r\\n    @     0x7f9bd7115ec0  caffe::CuDNNConvolutionLayer<>::Reshape()\\r\\n    @     0x7f9bd71eab62  caffe::Net<>::Init()\\r\\n    @     0x7f9bd71edb60  caffe::Net<>::Net()\\r\\n    @     0x7f9bd9102468  op::NetCaffe::initializationOnThread()\\r\\n    @     0x7f9bd91974cc  op::addCaffeNetOnThread()\\r\\n    @     0x7f9bd919879f  op::PoseExtractorCaffe::netInitializationOnThread()\\r\\n    @     0x7f9bd919db00  op::PoseExtractorNet::initializationOnThread()\\r\\n    @     0x7f9bd9194111  op::PoseExtractor::initializationOnThread()\\r\\n    @     0x7f9bd918e9b1  op::WPoseExtractor<>::initializationOnThread()\\r\\n    @     0x7f9bd9119aa1  op::Worker<>::initializationOnThreadNoException()\\r\\n    @     0x7f9bd9119be0  op::SubThread<>::initializationOnThread()\\r\\n    @     0x7f9bd911caf8  op::Thread<>::initializationOnThread()\\r\\n    @     0x7f9bd911ccfd  op::Thread<>::threadFunction()\\r\\n    @     0x7f9bd8a44b8e  (unknown)\\r\\n    @     0x7f9bd817c6ba  start_thread\\r\\n    @     0x7f9bd849951d  clone\\r\\n    @              (nil)  (unknown)\\r\\nAborted (core dumped)\\r\\n')]\n",
      "uesr: gineshidalgo99\n",
      "Title: Release version works but debug version does not - CUDA (7 vs. 0): too many resources requested\n",
      "Body: issue summary @zhaishengfu issue #13: > when i compile using debug mode, there are still errors with: terminate called after throwing an instance of 'std::runtimerelease -a` on ubuntu): distributor id: ubuntu description: ubuntu 14.04.3 lts release: 14.04 codename: trusty ** (`nvidia-smi`): gtx-1070 compiler (`gcc --version` on ubuntu): and my cpu is 4 core\n",
      "label: Error\n",
      "author_association: MEMBER\n",
      "comment_list: [('gineshidalgo99', 'MEMBER', '@zhaishengfu It should be fixed now. Please post again if the error persists. Thanks!')]\n",
      "uesr: ShihanWang\n",
      "Title: How can I use OpenPose in another project?\n",
      "Body: issue summary my test project: i just copy the file rtpose.cpp to the project.then i write a cmakelists.txt: cmakerequired(version 2.8) project(test) set(cmakecompiler \"g++\") set(cmaketype debug) set(cmakeflags \"-std=c++0x\") finddirectories( ${cudadirs} /home/wsh/projects/openpose/include /home/wsh/projects/openpose/3rdparty/caffe/include ) addlinklibs} /home/wsh/projects/openpose/build/lib/libopenpose.so /home/wsh/projects/openpose/3rdparty/caffe/build/lib/libcaffe.so ) i can run the examples successfully in openpose, but in my test project, it can not. type of issue help wanted openpose output (if any) /home/wsh/projects/openpose/include/openpose/experimental/face/faceextractor.hpp:48:29: error: ‘resizeandmergecaffe’ was not declared in this scope std::sharedptr spnmscaffe; ......\n",
      "label: question\n",
      "author_association: NONE\n",
      "comment_list: [('gineshidalgo99', 'MEMBER', 'This is from a Makefile file where I used the OpenPose binaries in a different project, I think you forgot to add the USE_CAFFE define in your cmake, you can get the required defines from my Makefile (there might be some duplication since I did it quickly, but it is working):\\r\\n\\r\\nPerform `make distribute -j8` on the OpenPose folder to get in `./distribute` the `include` and `lib` folders. The Caffe `lib` and `include` are already in `3rdparty/caffe/distribute` (used by OpenPose).\\r\\n\\r\\n### OpenPose (assuming its `include` and `lib` are located in ./3rdparty/openpose)\\r\\n`-Wl,-rpath=./3rdparty/openpose/lib -Wl,-Bdynamic -L./3rdparty/openpose/lib/ -lopenpose -DUSE_CAFFE`\\r\\n\\r\\n### Caffe (assuming its `include` and `lib` are located in ./3rdparty/caffe/distribute)\\r\\n`-Wl,-rpath=./3rdparty/caffe/distribute/lib -Wl,-Bdynamic -L./3rdparty/caffe/distribute/lib/ -lcaffe -DUSE_CUDNN`\\r\\n\\r\\n### OpenCV (add more OpenCV flags if you need them)\\r\\n`-lopencv_core -lopencv_highgui -lopencv_imgproc -lopencv_contrib -lopencv_calib3d`\\r\\n\\r\\n### CUDA, something like:\\r\\n`-I/usr/local/cuda-8.0/include/ -L/usr/local/cuda-8.0/lib64 -lcudart -lcublas -lcurand`\\r\\n\\r\\n### 3rdparty that Caffe uses\\r\\n`-lcudnn -lglog -lgflags -lboost_system -lboost_filesystem -lm -lboost_thread`\\r\\n\\r\\n### Other required\\r\\n`-pthread -fPIC -std=c++11 -fopenmp`\\r\\n\\r\\n### Optimization flags (optional)\\r\\n`-DNDEBUG -O3 -march=native`\\r\\n\\r\\nLet me know whether it works'), ('ShihanWang', 'NONE', 'Thank you very much.  I have solved the problem and I found that:\\r\\n1. In my CMakeLists.txt,  I need to change the Caffe directory:  SET(caffe_DIR ${OpenPose_DIR}/3rdparty/caffe/distribute);\\r\\n2. I delete \"#ifdef USE_CAFFE\"  in some files named as xxCaffe.hpp in openpose/include/.\\r\\nThen I make openpose again and everything is OK.  Without the 2nd step, I will have the same problem.But I do not know what is the effect of my change, maybe I just need to add USE_CAFFE in my CMakeLists.txt.'), ('gineshidalgo99', 'MEMBER', \"Thank you for posting it! Closing this then.\\r\\n\\r\\nEDITED: As an extra detail, instead of using this CMake example to construct the cmake file, I'd rather consider file example in the doc/installation_cmake.md document itself.\"), ('yorgosk', 'NONE', 'Hello!\\r\\n\\r\\nI am trying to use my current OpenPose installation for the compilation of another project (for the https://github.com/stevenjj/openpose_ros to be specific). I have build and tested my OpenPose installation as the documentation instructs me and everything seems fine, however I cannot do ```make distribute```. I have searched the documentation for a solution, but I am probably missing something.\\r\\nSince the instructions above are about one and a half year old @gineshidalgo99 can you please give me an update?\\r\\n\\r\\nThanks!'), ('gineshidalgo99', 'MEMBER', 'Hi, I believe doc/install.md has an explanation on how to include the library through CMake\\r\\n\\r\\nYou talk about `make distribute`, could you quickly send me a link to the instructions (in current master OP) where it says `make distribute`? So I can update that part.'), ('yorgosk', 'NONE', \"Okay, I'll check doc/install.md again.\\r\\n\\r\\nThe instructions in current master do not mention anything about make distribute, as far as I have read them. I first read about it here and I figured that since it used to be an option but apparently it is no more, I better ask you for a pointer to the thing that you substituted it with.\"), ('gineshidalgo99', 'MEMBER', 'OK yeah, this is based on the old basic installer, the new installer way (using CMake) provides much higher configuration settings, following how big libraries do it (e.g., OpenCV, Caffe, etc.).\\r\\n\\r\\nSo please, check https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/installation.md#openpose-from-other-projects-ubuntu-and-mac for all the details.'), ('burnzzz', 'NONE', 'https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/installation.md#openpose-from-other-projects-ubuntu-and-mac  is dead link.')]\n",
      "uesr: ufohuang98\n",
      "Title: Can this model tracking people or hand movement\n",
      "Body: i want to use this to make a gesture demo to control application or other iot. does it possible?\n",
      "label: other\n",
      "author_association: NONE\n",
      "comment_list: [('gineshidalgo99', 'MEMBER', 'Not for now, but we are planing to (as a LONG-term goal).\\r\\n\\r\\nWe have them both as pending extensions in our wait-list (hands should be added within 1-2 months, tracking we do not have an idea yet).\\r\\n\\r\\nThere are some methods for tracking, in case you are interested. You can take a look to #15.\\r\\n\\r\\nLet me know if you have any other question.'), ('ufohuang98', 'NONE', 'Thank you for your reply！'), ('heurainbow', 'NONE', 'I am going to implement a multi-person pose tracker. I find the hand tracker already implemented is quite helpful. However, I wonder how to acquire (or update) frame ID in class HandDetector. Can I simply make mCurrentId++ when calling updateTracker()? I am not familiar with the multi-threading framework, and I am not sure if this is the right way. @gineshidalgo99 '), ('gineshidalgo99', 'MEMBER', 'The hand tracker is not finished yet, but anyway it is meant for different purposes than tracking the same person across frames.\\r\\nI think the easiest way is to get the frame ID is by checking the (include/core/) Datum that is shared among the multi-threads. In particular, its `id` field. Since you will have to use the shared Datum anyway, using that ID should be the easiest. @heurainbow '), ('heurainbow', 'NONE', \"@gineshidalgo99 I wonder how a webcam producer or a video producer keep the frame order sequentially in a multi-gpu case. Since each frame is processed by one gpu, there is no guarantee that each frame is processed in order. But the gui does show images in order and I couldn't find the reason in the code.\"), ('gineshidalgo99', 'MEMBER', 'There is a internal buffer to keep the order'), ('heurainbow', 'NONE', '@gineshidalgo99 If possible, please show me which file or code I should refer to, and please give me a brief explanation.'), ('gineshidalgo99', 'MEMBER', 'This is the file, it simply uses the Datum ID to sort the frames\\r\\nhttps://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/include/openpose/thread/wQueueOrderer.hpp'), ('seovchinnikov', 'NONE', '@gineshidalgo99 what methods for tracking did you look at? \\r\\n'), ('gineshidalgo99', 'MEMBER', '@seovchinnikov We are currently working in an expensive but accurate body recognition algorithm and in a basic bust fast LK tracking algorithm. We are trying to add them by the end of the year. Thanks'), ('seovchinnikov', 'NONE', \"@gineshidalgo99 thank you,\\r\\nAs far as I understand, will body recognition algorithm' be based on basic body keypoints detector or will it be inderpendent end-to-end part to complement openpose library (smth like https://github.com/bendidi/Tracking-with-darkflow)?\\r\\n\\r\\n\"), ('gineshidalgo99', 'MEMBER', \"It'd be an extra component, that can be enabled (slower but with temporary information) or disabled (to keep the current OpenPose behavior)\"), ('superying', 'NONE', '@heurainbow Are there any progress in your multi-person pose tracker? Do you implement it base on the key points from openpose?')]\n"
     ]
    }
   ],
   "source": [
    "# data_path = \"/home/luomingkai/workspace/issue_llm/issue_classify/issue_with_comments_framework/matched_results_test.json\"\n",
    "data_path = \"/root/issue_classify/issue_with_comments_framework/matched_results_test_modify_other_update.json\"\n",
    "with open(data_path, encoding=\"utf-8\") as fp:\n",
    "    issue_data = json.load(fp)\n",
    "    for idx, data in enumerate(issue_data):\n",
    "        # print(issue_data)\n",
    "        uesr, title, body, label, author_association = data[\"user\"][\"login\"], data[\"title\"], data[\"body\"], data[\"tag_labels\"], data[\"author_association\"]\n",
    "        comment_list = data[\"comments_list\"]\n",
    "        if len(comment_list) > 0:\n",
    "            comment_list = [(com[\"user\"][\"login\"], com[\"author_association\"], com[\"body\"]) for com in comment_list]\n",
    "\n",
    "        print(f\"uesr: {uesr}\")\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Body: {body}\")\n",
    "        print(f\"label: {label}\")\n",
    "        print(f\"author_association: {author_association}\")\n",
    "        print(f\"comment_list: {comment_list}\")\n",
    "        \n",
    "        \n",
    "        # print()\n",
    "        if idx > 2:\n",
    "            break\n",
    "        # print(title, description, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa3f081-f81e-4b27-a382-785a7e6805d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a48483e6-5db1-473e-a7e6-48a835b628c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_issue_prompt = r\"\"\"\n",
    "### **Role**  \n",
    "You are an expert in GitHub repository analysis. Your task is to classify a given GitHub Issue into one of the following categories: **error**, **performance**, **deployment**, **question**, or **other**.\n",
    "\n",
    "Before analyzing the Issue conversation, you must carefully review the repository description to understand its intended functionality, scope, dependencies, and purpose. This understanding will help determine if the Issue is genuinely related to the repository’s code, configuration, or official documentation, or if it stems from user misunderstandings, local environment misconfigurations, external dependencies, or unrelated factors.\n",
    "\n",
    "Analyze the conversation context, the repository’s description, and the roles of participants to determine the correct classification. For each round of the conversation, provide an intermediate classification and reasoning. At the end, combine all intermediate results to produce a final classification, using a **self-reflection** mechanism to validate your reasoning and ensure your classification aligns with the root cause of the Issue. If the issue is found to be completely unrelated to the repository based on its description, it must be categorized as **other**.\n",
    "\n",
    "\n",
    "### Repository Description  \n",
    "<user_provided_repository_description>\n",
    "\n",
    "Use this description as a reference point throughout your classification process. Compare the reported issues, errors, or requests against the repository’s stated purpose, functionality, and supported features. If the conversation’s content or the user’s requests fall outside the scope defined by the repository description, consider classifying the issue as **other**. If misunderstandings occur because the user did not follow or comprehend instructions reflected in the repository description or official documentation, consider whether it should be classified as **question**. Ensure that “deployment” issues are directly related to repository code or its official documentation, not external factors.\n",
    "\n",
    "\n",
    "### **Participants**  \n",
    "1. **Issue Raiser**: Describes the problem, requests a feature, or raises concerns related to the repository.  \n",
    "2. **Commenter**: Provides insights, shares similar experiences, or suggests possible solutions.  \n",
    "3. **MEMBER**: Evaluates technical feasibility, assigns tasks, or progresses the Issue.  \n",
    "4. **COLLABORATOR**: Proposes solutions, performs in-depth analysis, or submits Pull Requests.  \n",
    "5. **CONTRIBUTOR**: Offers historical context, helps reproduce issues, or validates fixes.  \n",
    "6. **OWNER**: Makes final decisions, prioritizes tasks, or proposes long-term resolutions.\n",
    "\n",
    "\n",
    "### **Issue Categories**  \n",
    "\n",
    "- **error**:  \n",
    "  Problems directly stemming from the repository’s code, configuration, or inherent incompatibilities within the repository. For example, runtime errors, exceptions, or failures in the repository code itself.\n",
    "\n",
    "- **performance**:  \n",
    "  Issues where the repository’s code or configuration leads to slow execution times, bottlenecks, or inefficient resource usage.\n",
    "\n",
    "- **deployment**:  \n",
    "  Issues specifically caused by the repository’s code or incorrect/inadequate documentation during the installation or deployment process.\n",
    "    - **Scope Validation**:  \n",
    "      Reference the repository description to ensure that the reported deployment problem is within the scope of the repository’s intended setup process. Deployment issues must stem from defects or omissions in the repository’s code, configuration, or documentation.\n",
    "    \n",
    "    - **Exclusions**:  \n",
    "      If the issue is caused by any of the following, it should be classified as **question**:  \n",
    "      - User mistakes, such as missing or incorrectly installed dependencies (e.g., wrong CUDA driver version, missing required libraries, or incorrect installation steps).  \n",
    "      - Using outdated tools (e.g., an older CMake version than required).  \n",
    "      - Hardware or environment limitations (e.g., using unsupported GPUs).  \n",
    "\n",
    "- **question**:  \n",
    " Issues arising from user misunderstandings, failure to follow documented instructions, incorrect usage, or local environment misconfigurations that are not caused by the repository code or official documentation. \n",
    " - These issues can often be resolved by following existing guidance. \n",
    " - Additionally, users may raise questions or engage in discussions with developers regarding specific usage scenarios.\n",
    "\n",
    "- **other**:  \n",
    "  Issues that do not fall into predefined categories, including topics unrelated to the repository as determined by comparing the issue to the repository description. This category also includes user requests for feature enhancements or pull requests that go beyond the scope or goals of the repository. Additionally, it covers discussions related to improvements in documentation, user experience suggestions, community governance, collaboration processes, and strategic or technical proposals that are not directly related to code or documentation errors.\n",
    "  1. Topics that do not fit the predefined categories.  \n",
    "  2. Issues or discussions unrelated to the repository, as determined by comparing the Issue to the repository description.  \n",
    "  3. Feature suggestions or requests beyond the repository’s described scope.\n",
    "\n",
    "  **Examples of “other”** include:  \n",
    "  - **Feature Suggestions and Testing**: New feature ideas or enhancements not supported by current repository goals.  \n",
    "  - **Documentation and Resources**: Improvements or additions to general documentation, tutorials, or references that are not about fixing a code-related or doc-related deployment bug.  \n",
    "  - **User Experience**: Suggestions to improve usability, design, or general compatibility.  \n",
    "  - **Community and Collaboration**: Ideas about community governance, contribution processes, or engagement.  \n",
    "  - **Strategic and Technical Proposals**: Infrastructure, policy, or long-term goal considerations unrelated to immediate code or documentation errors.  \n",
    "  - **Miscellaneous Topics**: Relevant issues that do not fit other categories or are unrelated submissions.\n",
    "\n",
    "\n",
    "### **Process**  \n",
    "**The process must be strictly executed.**\n",
    "1. **Read the Conversation and Repository Description**:  \n",
    "   Begin by reviewing the repository description thoroughly to understand the project’s purpose, supported features, and scope. Keep this context in mind as you examine each round of the conversation. If the problem discussed clearly falls outside the repository’s described capabilities or instructions, consider “other” or “question.”\n",
    "\n",
    "2. **For Each Round**:  \n",
    "   - Classify the round into one of the five categories: **error**, **performance**, **deployment**, **question**, or **other**.  \n",
    "   - Provide a JSON object with keys `round`, `classification`, `evidence`, and `self_reflection`.  \n",
    "     - **round**: The round number.  \n",
    "     - **classification**: Your chosen classification for that round.  \n",
    "     - **evidence**: Direct quotes or references from that specific round’s conversation that support your classification.  \n",
    "     - **self_reflection**: Justify your classification’s accuracy. Confirm whether it matches the repository’s description and consider alternatives. If unsure, reflect on the possibility of user misunderstanding vs. repository code issues.\n",
    "\n",
    "3. **Final Self-Reflection Across Rounds**:  \n",
    "   - After classifying all rounds, review the entire conversation and your intermediate classifications.  \n",
    "   - Check for consistency:  \n",
    "     - Does the chosen final category align with the majority of evidence and the repository description?  \n",
    "     - Are there any contradictions between rounds?  \n",
    "     - Could the issue be a misunderstanding (question) rather than a repository problem (error/performance/deployment)?  \n",
    "     - If the issue is irrelevant to the repository description, finalize as **other**.\n",
    "\n",
    "4. **Finalize the Classification**:  \n",
    "   Produce a final JSON object containing:  \n",
    "   - `final_issue_type`: Your chosen final category.  \n",
    "   - `combined_evidence`: A concise summary of the entire conversation supporting your final classification.  \n",
    "   - `final_self_reflection`: Analysis of the reasoning process, conflicts, and how you ensured consistency with the repository description and instructions.\n",
    "\n",
    "\n",
    "### **Output Format**\n",
    "\n",
    "**Intermediate Results for Each Round**:\n",
    "```json\n",
    "{\n",
    "    \"round\": <Round Number>,\n",
    "    \"classification\": \"<error|performance|deployment|question|other>\",\n",
    "    \"evidence\": \"<Evidence from the current round of the conversation>\",\n",
    "    \"self_reflection\": \"<Analysis of classification accuracy and consistency for this round>\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Final Combined Output**:\n",
    "```json\n",
    "{\n",
    "    \"final_issue_type\": \"<error|performance|deployment|question|other>\",\n",
    "    \"combined_evidence\": \"<Concise summary of the conversation supporting the final classification>\",\n",
    "    \"final_self_reflection\": \"<Analysis of conflicts, shifts, and consistency across all rounds>\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Validation Reminder**:  \n",
    "- Ensure that all JSON objects are valid and correctly formatted.  \n",
    "- The keys should be correctly quoted, and all values should be properly formatted strings or numbers where applicable.  \n",
    "- No extra or missing fields are allowed.  \n",
    "- Confirm that the classification aligns with the repository description, the issue categories, and the dialogue evidence.\n",
    "\n",
    "\n",
    "### **Examples**\n",
    "\n",
    "#### **Example 1: Error with Contributor’s Historical Context**\n",
    "*Conversation:*  \n",
    "- **Issue Raiser**: \"Running model.fit() raises a KeyError related to missing labels in the dataset.\"  \n",
    "- **CONTRIBUTOR**: \"This issue might be related to an earlier change in #45 that introduced stricter label validation.\"  \n",
    "- **COLLABORATOR**: \"We’ll patch data_loader.py to handle missing labels.\"  \n",
    "- **OWNER**: \"The fix is merged. Let us know if it resolves the problem.\"\n",
    "\n",
    "*Intermediate Results:*  \n",
    "```json\n",
    "{\n",
    "    \"round\": 1,\n",
    "    \"classification\": \"error\",\n",
    "    \"evidence\": \"The Issue Raiser described a KeyError from the repository code handling labels.\",\n",
    "    \"self_reflection\": \"The classification as error is justified because the bug originates in the repository code.\"\n",
    "}\n",
    "```\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"round\": 2,\n",
    "    \"classification\": \"error\",\n",
    "    \"evidence\": \"The CONTRIBUTOR referenced a past commit that made label validation stricter.\",\n",
    "    \"self_reflection\": \"This supports the error classification. The issue traces back to a known code change.\"\n",
    "}\n",
    "```\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"round\": 3,\n",
    "    \"classification\": \"error\",\n",
    "    \"evidence\": \"The COLLABORATOR proposed a code fix for handling missing labels.\",\n",
    "    \"self_reflection\": \"The solution involves changing repository code, reinforcing that it is an error.\"\n",
    "}\n",
    "```\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"round\": 4,\n",
    "    \"classification\": \"error\",\n",
    "    \"evidence\": \"The OWNER merged a fix into the repository.\",\n",
    "    \"self_reflection\": \"All evidence is consistent with classifying the issue as error.\"\n",
    "}\n",
    "```\n",
    "\n",
    "*Final Combined Output:*  \n",
    "```json\n",
    "{\n",
    "    \"final_issue_type\": \"error\",\n",
    "    \"combined_evidence\": \"The repository code did not handle missing labels, causing a KeyError. Historical context and a code fix confirm it as an error.\",\n",
    "    \"final_self_reflection\": \"All rounds consistently supported the error classification. The solution required a code change in the repository.\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### **Example 2: Question Misclassified Initially**\n",
    "*Conversation:*  \n",
    "- **Issue Raiser**: \"I tried running the code, but the output looks weird. Is this a bug?\"  \n",
    "- **MEMBER**: \"Have you verified if the input data matches the format described in the README?\"  \n",
    "- **Issue Raiser**: \"I missed the formatting instructions. After fixing the input, it works fine.\"\n",
    "\n",
    "*Intermediate Results:*  \n",
    "```json\n",
    "{\n",
    "    \"round\": 1,\n",
    "    \"classification\": \"error\",\n",
    "    \"evidence\": \"The Issue Raiser suspected a bug due to unexpected output.\",\n",
    "    \"self_reflection\": \"Initial classification as error is tentative. This could be a user misunderstanding.\"\n",
    "}\n",
    "```\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"round\": 2,\n",
    "    \"classification\": \"question\",\n",
    "    \"evidence\": \"The MEMBER directed the user to check input format per the documentation.\",\n",
    "    \"self_reflection\": \"The reclassification to question is justified since the problem may be due to user input errors, not the code.\"\n",
    "}\n",
    "```\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"round\": 3,\n",
    "    \"classification\": \"question\",\n",
    "    \"evidence\": \"The Issue Raiser fixed the input format and the problem was resolved.\",\n",
    "    \"self_reflection\": \"The resolution confirms it as a question caused by user misunderstanding, not an error in the repository.\"\n",
    "}\n",
    "```\n",
    "\n",
    "*Final Combined Output:*  \n",
    "```json\n",
    "{\n",
    "    \"final_issue_type\": \"question\",\n",
    "    \"combined_evidence\": \"The Issue was due to incorrect input formatting. After following the documentation, the user resolved the problem.\",\n",
    "    \"final_self_reflection\": \"The initial misclassification was corrected. No repository code changes were needed; it was a usage question.\"\n",
    "}\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 仅使用第一条issue的模版\n",
    "question_prompt = \"\"\"\n",
    "````\n",
    "*Conversation*:\n",
    "- {}: ### Title: \"{}\" ### Body: \"{}\"\n",
    "````\n",
    "\"\"\"\n",
    "\n",
    "# 使用comment的版本\n",
    "question_comment_prompt = \"\"\"\n",
    "*Conversation*:\n",
    "- **{}**: \"{} {}\"\n",
    "\"\"\"\n",
    "\n",
    "comment_prompt = \"\"\"\n",
    "- **{}**: \"{}\"\n",
    "\"\"\"\n",
    "\n",
    "ROLE_MAP_BEGIN = {\n",
    "    \"NONE\": \"ISSUE RAISER\",\n",
    "    \"MEMBER\": \"MEMBER\",\n",
    "    \"COLLABORATOR\": \"COLLABORATOR\",\n",
    "    \"CONTRIBUTOR\": \"CONTRIBUTOR\",\n",
    "    \"OWNER\": \"OWNER\"\n",
    "}\n",
    "\n",
    "ROLE_MAP_COMMENT = {\n",
    "    \"NONE\": \"Commenter\",\n",
    "    \"MEMBER\": \"MEMBER\",\n",
    "    \"COLLABORATOR\": \"COLLABORATOR\",\n",
    "    \"CONTRIBUTOR\": \"CONTRIBUTOR\",\n",
    "    \"OWNER\": \"OWNER\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9993b69-c200-4ea9-bdf4-c610e1ba7810",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DESC = {\n",
    "    \"CMU-Perceptual-Computing-Lab/openpose\": {\n",
    "        \"description\": \"OpenPose is a real-time multi-person keypoint detection library developed by the Carnegie Mellon Perceptual Computing Lab. It estimates human body, face, hands, and foot keypoints from single images, providing 2D real-time multi-person keypoint detection, including 15, 18, or 25-keypoint body/foot keypoint estimation, 2x21-keypoint hand keypoint estimation, and 70-keypoint face keypoint estimation. Additionally, it offers 3D real-time single-person keypoint detection and a calibration toolbox. OpenPose is compatible with various operating systems, including Ubuntu, Windows, and macOS, and supports CUDA (Nvidia GPU), OpenCL (AMD GPU), and CPU-only versions.\",\n",
    "        \"url\": \"https://github.com/CMU-Perceptual-Computing-Lab/openpose\"\n",
    "    },\n",
    "    \"CorentinJ/Real-Time-Voice-Cloning\": {\n",
    "        \"description\": \"Real-Time Voice Cloning is a Python-based tool that enables the cloning of voices in real-time. It utilizes deep learning models to synthesize speech that mimics a target voice, requiring only a few seconds of audio from the desired speaker. The repository provides code and instructions to train and use the voice cloning system.\",\n",
    "        \"url\": \"https://github.com/CorentinJ/Real-Time-Voice-Cloning\"\n",
    "    },\n",
    "    \"JaidedAI/EasyOCR\": {\n",
    "        \"description\": \"EasyOCR is an open-source Optical Character Recognition (OCR) library that supports over 80 languages. It is designed to be easy to use and provides accurate text recognition from images. The library is built on PyTorch and offers pre-trained models for various languages and scripts.\",\n",
    "        \"url\": \"https://github.com/JaidedAI/EasyOCR\"\n",
    "    },\n",
    "    \"deepfakes/faceswap\": {\n",
    "        \"description\": \"Faceswap is a deep learning-based tool for face-swapping in images and videos. It allows users to train models to swap faces between different subjects, providing a platform for experimenting with deepfake technology. The repository includes code for training and using the face-swapping models.\",\n",
    "        \"url\": \"https://github.com/deepfakes/faceswap\"\n",
    "    },\n",
    "    \"deezer/spleeter\": {\n",
    "        \"description\": \"Spleeter is an open-source tool developed by Deezer for source separation in music tracks. It uses deep learning models to separate audio into stems, such as vocals and accompaniment, enabling applications like karaoke and remixing. The repository provides pre-trained models and code for audio source separation.\",\n",
    "        \"url\": \"https://github.com/deezer/spleeter\"\n",
    "    },\n",
    "    \"dusty-nv/jetson-inference\": {\n",
    "        \"description\": \"Jetson Inference is a collection of deep learning inference samples and models for NVIDIA Jetson devices. It includes code for image classification, object detection, and segmentation, optimized for Jetson hardware. The repository provides pre-trained models and examples to demonstrate the capabilities of Jetson devices in AI applications.\",\n",
    "        \"url\": \"https://github.com/dusty-nv/jetson-inference\"\n",
    "    },\n",
    "    \"iperov/DeepFaceLab\": {\n",
    "        \"description\": \"DeepFaceLab is a deep learning tool for creating deepfakes, focusing on face-swapping in videos. It provides a comprehensive set of tools for training and applying deep learning models to perform face-swapping tasks. The repository includes code for data preparation, model training, and face-swapping applications.\",\n",
    "        \"url\": \"https://github.com/iperov/DeepFaceLab\"\n",
    "    },\n",
    "    \"junyanz/pytorch-CycleGAN-and-pix2pix\": {\n",
    "        \"description\": \"This repository provides PyTorch implementations of CycleGAN and pix2pix, two popular models for image-to-image translation tasks. CycleGAN enables image translation without paired examples, while pix2pix requires paired images for training. The repository includes code and pre-trained models for various image translation tasks.\",\n",
    "        \"url\": \"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\"\n",
    "    },\n",
    "    \"mozilla/TTS\": {\n",
    "        \"description\": \"Mozilla TTS is an open-source Text-to-Speech (TTS) engine that aims to make speech synthesis more accessible. It provides implementations of state-of-the-art TTS models, including Tacotron and FastSpeech, and supports training on custom datasets. The repository includes code for training and using TTS models.\",\n",
    "        \"url\": \"https://github.com/mozilla/TTS\"\n",
    "    },\n",
    "    \"streamlit/streamlit\": {\n",
    "        \"description\": \"Streamlit is an open-source app framework for Machine Learning and Data Science projects. It allows users to create interactive web applications for data analysis and visualization with minimal code. The repository provides the core framework and examples for building Streamlit applications.\",\n",
    "        \"url\": \"https://github.com/streamlit/streamlit\"\n",
    "    },\n",
    "    \"microsoft/recommenders\": {\n",
    "        \"description\": \"Recommenders is a project under the Linux Foundation of AI and Data. This repository contains examples and best practices for building recommendation systems, provided as Jupyter notebooks. The examples detail learnings on five key tasks: preparing and loading data for each recommendation algorithm, building models using various classical and deep learning recommendation algorithms such as Alternating Least Squares (ALS) or eXtreme Deep Factorization Machines (xDeepFM), evaluating algorithms with offline metrics, tuning and optimizing hyperparameters for recommendation models, and operationalizing models in a production environment on Azure. Several utilities are provided to support common tasks such as loading datasets in the format expected by different algorithms, evaluating model outputs, and splitting training/test data. Implementations of several state-of-the-art algorithms are included for self-study and customization in your own applications.\",\n",
    "        \"url\": \"https://github.com/recommenders-team/recommenders\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a23fcb-bb7c-4a19-9e4e-337671f8f7c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (36478 > 16384). Running this sequence through the model will result in indexing errors\n",
      "Processed prompts: 100%|██████████| 1933/1933 [22:26<00:00,  1.44it/s, est. speed input: 5150.19 toks/s, output: 481.26 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.41      0.23      0.30       251\n",
      " performance       0.19      0.33      0.24        60\n",
      "  deployment       0.33      0.10      0.16       165\n",
      "    question       0.58      0.71      0.64       921\n",
      "       other       0.36      0.35      0.35       536\n",
      "\n",
      "    accuracy                           0.48      1933\n",
      "   macro avg       0.38      0.35      0.34      1933\n",
      "weighted avg       0.47      0.48      0.46      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1933/1933 [22:42<00:00,  1.42it/s, est. speed input: 5089.93 toks/s, output: 473.21 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.39      0.23      0.29       251\n",
      " performance       0.18      0.33      0.23        60\n",
      "  deployment       0.33      0.11      0.16       165\n",
      "    question       0.59      0.73      0.65       921\n",
      "       other       0.40      0.36      0.38       536\n",
      "\n",
      "    accuracy                           0.50      1933\n",
      "   macro avg       0.38      0.35      0.34      1933\n",
      "weighted avg       0.48      0.50      0.47      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1933/1933 [22:46<00:00,  1.41it/s, est. speed input: 5074.87 toks/s, output: 472.77 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.39      0.22      0.28       251\n",
      " performance       0.19      0.35      0.25        60\n",
      "  deployment       0.34      0.12      0.17       165\n",
      "    question       0.58      0.72      0.64       921\n",
      "       other       0.37      0.34      0.36       536\n",
      "\n",
      "    accuracy                           0.49      1933\n",
      "   macro avg       0.38      0.35      0.34      1933\n",
      "weighted avg       0.47      0.49      0.46      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1933/1933 [22:55<00:00,  1.40it/s, est. speed input: 5041.21 toks/s, output: 474.22 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.38      0.21      0.27       251\n",
      " performance       0.22      0.37      0.28        60\n",
      "  deployment       0.34      0.10      0.16       165\n",
      "    question       0.60      0.73      0.66       921\n",
      "       other       0.37      0.36      0.37       536\n",
      "\n",
      "    accuracy                           0.50      1933\n",
      "   macro avg       0.38      0.35      0.35      1933\n",
      "weighted avg       0.47      0.50      0.47      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1933/1933 [23:14<00:00,  1.39it/s, est. speed input: 4974.22 toks/s, output: 468.65 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.39      0.24      0.29       251\n",
      " performance       0.22      0.40      0.29        60\n",
      "  deployment       0.38      0.11      0.17       165\n",
      "    question       0.59      0.72      0.65       921\n",
      "       other       0.39      0.37      0.38       536\n",
      "\n",
      "    accuracy                           0.50      1933\n",
      "   macro avg       0.40      0.37      0.36      1933\n",
      "weighted avg       0.48      0.50      0.48      1933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  13%|█▎        | 249/1933 [03:27<15:53,  1.77it/s, est. speed input: 4377.82 toks/s, output: 402.62 toks/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "TIMES = 10\n",
    "experiments = []\n",
    "# 保存所有实验结果的列表\n",
    "all_reports = []\n",
    "\n",
    "data_path = \"/root/issue_classify/issue_with_comments_framework/matched_results_test_modify_other_update.json\"\n",
    "result_path = \"/root/autodl-fs/log/deepseek_v2_lite_chat_agent.xlsx\"\n",
    "\n",
    "for t in range(TIMES):\n",
    "    origin_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    join_index = []\n",
    "    with open(data_path, encoding=\"utf-8\") as fp:\n",
    "        issue_data = json.load(fp)\n",
    "        taged_data = issue_data\n",
    "        all_messages = []\n",
    "        for idx, data in enumerate(issue_data):\n",
    "            raise_user, title, body, label, author_association = data[\"user\"][\"login\"], data[\"title\"], data[\"body\"], data[\"tag_labels\"], data[\"author_association\"]\n",
    "    \n",
    "            url = data[\"html_url\"]\n",
    "            match = re.search(r\"github\\.com/([^/]+/[^/]+)\", url)\n",
    "            repository = match.group(1)\n",
    "            repo_desc = REPO_DESC[repository][\"description\"]\n",
    "            \n",
    "            join_index.append(idx)\n",
    "            \n",
    "            comment_list = data[\"comments_list\"]\n",
    "            if len(comment_list) > 0:\n",
    "                comment_list = [(com[\"user\"][\"login\"], com[\"author_association\"], com[\"body\"]) for com in comment_list]\n",
    "    \n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": global_issue_prompt.replace(\"<user_provided_repository_description>\", f\"The repository {repo_desc}\")\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question_comment_prompt.format(\n",
    "                        ROLE_MAP_BEGIN[str(author_association)],\n",
    "                        title, \n",
    "                        body\n",
    "                        )\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            origin_labels.append(label)\n",
    "    \n",
    "            for user, author_association, body in comment_list:\n",
    "                if user == raise_user:\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": comment_prompt.format(\n",
    "                                ROLE_MAP_BEGIN[str(author_association)],\n",
    "                                body\n",
    "                                )\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": comment_prompt.format(\n",
    "                                ROLE_MAP_COMMENT[str(author_association)],\n",
    "                                # user,\n",
    "                                body\n",
    "                                )\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "            all_messages.append(messages)\n",
    "        \n",
    "        responses = get_deepseek_output(model, tokenizer, all_messages, max_tokens=4096)\n",
    "        tmp_origin_labels = []\n",
    "        for idx, response in enumerate(responses):\n",
    "            cls_result = extract_all_json_from_string(response)\n",
    "            # print(f\"label: {origin_labels[idx]}\")\n",
    "            # print(f\"response: {response}\")\n",
    "            # print(f\"cls_result: {cls_result}\")\n",
    "    \n",
    "            if cls_result:\n",
    "                found = False\n",
    "                for cls in cls_result:\n",
    "                    if cls.get(\"final_issue_type\"):\n",
    "                        result = cls.get(\"final_issue_type\")\n",
    "                        tmp_origin_labels.append(origin_labels[idx])\n",
    "                        pred_labels.append(result)\n",
    "                        found = True\n",
    "                        break\n",
    "                \n",
    "                # 对话提前结束，按照最后一轮对话的分类来判断\n",
    "                if not found:\n",
    "                    if cls_result[-1].get(\"classification\"):\n",
    "                        result = cls.get(\"classification\")\n",
    "                        tmp_origin_labels.append(origin_labels[idx])\n",
    "                        pred_labels.append(result)\n",
    "                        found = True\n",
    "                        \n",
    "                if not found:\n",
    "                    tmp_origin_labels.append(origin_labels[idx])\n",
    "                    pred_labels.append(\"other\")           \n",
    "            else:\n",
    "                tmp_origin_labels.append(origin_labels[idx])\n",
    "                pred_labels.append(\"other\")\n",
    "        origin_labels = tmp_origin_labels\n",
    "\n",
    "    origin_labels = [x.lower() for x in origin_labels]\n",
    "    pred_labels = [x.lower() for x in pred_labels]\n",
    "    tmp_pred_labels = []\n",
    "    for x in pred_labels:\n",
    "        if x not in [\"error\", \"performance\", \"deployment\", \"question\", \"other\"]:\n",
    "            x = 'other'\n",
    "        tmp_pred_labels.append(x)\n",
    "    pred_labels = tmp_pred_labels\n",
    "    \n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    labels = [\"error\", \"performance\", \"deployment\", \"question\", \"other\"]\n",
    "    label_map = {\n",
    "        \"error\": 0, \n",
    "        \"performance\": 1,\n",
    "        \"deployment\": 2,\n",
    "        \"question\": 3,\n",
    "        \"other\": 4\n",
    "    }\n",
    "    final_origin_labels_num = [label_map[l] for l in origin_labels]\n",
    "    final_pred_labels_num = [label_map[l] for l in pred_labels]\n",
    "    \n",
    "    report = classification_report(final_origin_labels_num, final_pred_labels_num, target_names=labels)\n",
    "    print(report)\n",
    "    report_dict = classification_report(final_origin_labels_num, final_pred_labels_num, target_names=labels, output_dict=True)\n",
    "    # 将字典转为 DataFrame\n",
    "    df_report = pd.DataFrame(report_dict).transpose()\n",
    "    df_report[\"experiment_id\"] = t + 1  # 添加实验编号\n",
    "    df_report.index.name = \"category\"\n",
    "    df_report.reset_index(inplace=True)\n",
    "    \n",
    "    all_reports.append(df_report)\n",
    "    all_reports.append(pd.DataFrame({\"category\": [\"\"]}))  # 添加空行\n",
    "\n",
    "final_df = pd.concat(all_reports, ignore_index=True)\n",
    "final_df.to_excel(result_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93caf7f2-8176-4b9f-a378-2b5b05421676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# 假设你的 vllm 模型对象是 `model`\n",
    "del model  # 删除模型对象\n",
    "torch.cuda.empty_cache()  # 清空 GPU 的缓存\n",
    "gc.collect()  # 强制进行垃圾回收\n",
    "\n",
    "import sys\n",
    "# 删除 `vllm` 和相关依赖\n",
    "del sys.modules[\"vllm\"]\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732888e0-6333-455f-9e82-5a6418610ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd87993-0d47-49e1-8c63-6b2ee1d7f0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd6d514-685c-44b5-974b-3cb0906aa4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be666c99-00d1-4be0-9606-f990318fc8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'question': 1081, 'other': 565, 'error': 141, 'performance': 94, 'deployment': 47, 'feature_request': 2, 'compilation/installation error': 1, 'documentation_error': 1, 'Other': 1})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       error       0.39      0.22      0.28       251\n",
      " performance       0.22      0.35      0.27        60\n",
      "  deployment       0.38      0.11      0.17       165\n",
      "    question       0.60      0.70      0.65       921\n",
      "       other       0.38      0.40      0.39       536\n",
      "\n",
      "    accuracy                           0.50      1933\n",
      "   macro avg       0.39      0.36      0.35      1933\n",
      "weighted avg       0.48      0.50      0.48      1933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all question类别打标之后的\n",
    "from collections import Counter\n",
    "print(Counter(pred_labels))\n",
    "origin_labels = [x.lower() for x in origin_labels]\n",
    "pred_labels = [x.lower() for x in pred_labels]\n",
    "tmp_pred_labels = []\n",
    "for x in pred_labels:\n",
    "    if x not in [\"error\", \"performance\", \"deployment\", \"question\", \"other\"]:\n",
    "        x = 'other'\n",
    "    tmp_pred_labels.append(x)\n",
    "pred_labels = tmp_pred_labels\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "labels = [\"error\", \"performance\", \"deployment\", \"question\", \"other\"]\n",
    "label_map = {\n",
    "    \"error\": 0, \n",
    "    \"performance\": 1,\n",
    "    \"deployment\": 2,\n",
    "    \"question\": 3,\n",
    "    \"other\": 4\n",
    "}\n",
    "final_origin_labels_num = [label_map[l] for l in origin_labels]\n",
    "final_pred_labels_num = [label_map[l] for l in pred_labels]\n",
    "\n",
    "report = classification_report(final_origin_labels_num, final_pred_labels_num, target_names=labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3b29a-d293-4712-8530-3cc32c24170e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af5545-f547-48e2-b6b2-7831dcb4825a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54813fac-96ea-42ea-8f02-d892dd5d4119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e439e41-30f0-4fb2-8edd-629f0c190bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39734c23-c8ae-4808-8cf8-020a5bfd6f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
